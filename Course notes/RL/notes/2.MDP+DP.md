
# MP-MRP-MDP
马尔可夫过程MP$\rightarrow$ 马尔可夫奖励过程MRP $\rightarrow$ 马尔可夫决策过程
递进关系：马尔可夫过程描述了状态之间的转移，马尔可夫奖励过程引入了奖励，而马尔可夫决策过程则进一步考虑了决策者在每个状态下选择动作的问题。
![[Pasted image 20231119204728.png]]
学习资料：
【【强化学习】从零开始推导贝尔曼最优方程】[[RL-1.pdf]],视频如下：
https://www.bilibili.com/video/BV1fN41197ES?vd_source=5b88bb66a193995c22ba142ffc69cff8
https://zhuanlan.zhihu.com/p/421406358


## 1. **马尔可夫过程（MP）:**
马尔可夫过程（Markov Process）是一种随机过程，其基本特性是具有马尔可夫性质，即**未来的状态只依赖于当前状态，而与过去的状态无关**。
   
   对于离散时间的马尔可夫过程，其状态转移概率满足马尔可夫性质，即未来状态只依赖于当前状态，即一步一步来：
$$p(S_{t+1}|S_t)=p(S_{t+1}|S_0,S_1,S_2,\cdots,S_t)$$

即可以写成下述式子，这里**只考虑一个状态到下一个状态**：
$$p(s'|s)=p(S_{t+1}=s'|S_t=s)$$

具体转移过程：
![[Pasted image 20231119203343.png]]

于是有了状态转移矩阵：
### **状态转移矩阵（State Transition Matrix）：**
   一个描述状态$s$ 之间转移概率的矩阵。如果有 $N$ 个状态，那么状态转移矩阵 $P$ 的元素 $P_{s,s'}$ 表示在当前处于状态 $s$ 时，下一步转移到状态 $s'$ 的概率。状态转移矩阵通常表示为：
   $$P = \begin{bmatrix} P_{1,1} & P_{1,2} & \ldots & P_{1,N} \\ P_{2,1} & P_{2,2} & \ldots & P_{2,N} \\ \vdots & \vdots & \ddots & \vdots \\ P_{N,1} & P_{N,2} & \ldots & P_{N,N} \end{bmatrix}$$


## 2. **马尔可夫奖励过程（MRP）:**

马尔可夫奖励过程（Markov Reward Process）是马尔可夫过程的扩展，引入了**奖励**的概念。在马尔可夫奖励过程中，每个状态转移都伴随着一个奖励，表示在从一个状态转移到另一个状态时所获得的**即时回报**。
### **奖励$R_{s}$**
马尔可夫奖励过程是对马尔可夫过程的扩展，引入了奖励函数 $R$。在离散时间下，其定义为：
   $$R_s = \mathbb{E}[R_{t+1} | S_t=s]$$
其中，$R_s$是在$t$ 时刻，状态 $s$ 时获得的奖励，$\mathbb{E}$ 表示期望。

思考一下：为什么使用期望？
	相当于$t$ 时刻给定状态$s$ ，转移到下一个时刻的奖励有很多种，所以采取一个期望。

### **折扣因子$\gamma$** 
用符号 $\gamma$ 表示，用于衡量未来奖励的重要性。折扣因子在 $0$ 到 $1$ 之间，越接近 $1$ 表示越重视未来奖励。


### **累计回报$G_t$**
在马尔可夫决策过程中，回报是决策者从起始状态开始，执行一系列动作并遵循某个策略$\pi()$后所获得的累积奖励。回报通常用 $G_t$ 表示，在时间步 $t$ 开始的回报定义为：
$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

### **状态值函数V**
状态值函数表示在某个状态下，特定策略下所能获得的期望累积回报。(主要是为了**评估**这个状态好不好)
对于状态 $s$ 的值函数表示为 $V(s)$。
具体而言，$V(s)$ 是从状态 $s$ 开始，所获得的期望回报。

$$ V = \mathbb{E} \left[ G_t \mid S_t = s \right] $$

其中，$G_t$ 是从时刻 $t$ 开始的累积回报，$\mathbb{E}_\pi$ 表示期望。


### **V的贝尔曼方程**

>**贝尔曼方程：** 贝尔曼方程描述了当前状态值函数与后续状态值函数之间的关系。它是在考虑特定策略的情况下的值函数更新关系。

通俗讲，就是类似去寻找 $f(n)$ 和$f(n+1)$ 的关系。


**推导：状态值函数➡贝尔曼方程**：
考虑状态值函数与下一个状态值函数之间的关系。
$$
V= \mathbb{E}\left[ G_t \mid S_t = s \right]\tag{状态值函数}
$$
$$
V= \mathbb{E}_\pi \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots \mid S_t = s \right]\tag{将$G_t$代入}
$$

转化为当前奖励 $R_{t+1}$ 加上未来的累积回报：

$$
V = \mathbb{E}_\pi \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right]
$$

运用期望公式分离：
$$
V= \mathbb{E}\left[R_{t+1}|S_t=s\right]+\gamma\mathbb{E}\left[G_{t+1}\mid S_t=s\right] 
$$

又因为$R_s = \mathbb{E}[R_{t+1} | S_t=s]$，则：
$$
V = R_s+\gamma\mathbb{E}\left[G_{t+1}\mid S_t=s\right] 
$$
将 $G_{t+1}$ 进一步展开为下一个时刻的状态值函数(如何展开请见下文)，即：

**状态值函数的贝尔曼方程**：
$$
V= R_s+\gamma\mathbb{E}\left[V(S_{t+1}) \mid S_t = s \right]
$$
表示在策略 $\pi$ 下，当前状态 $s$ 的值等于当前时刻获得的奖励 $R_{t+1}$ 加上折扣因子乘以下一状态 $S_{t+1}$ 的值的期望。

***
这里最后两个的公式转换，我们可以看到:$$\mathbb{E}\left[G_{t+1}\mid S_t=s\right] == \mathbb{E}\left[V(S_{t+1}) \mid S_t = s \right]$$
这里**仍需推导**。

- 两个前提：
>全期望公式（1）：$$\mathbb{E}[X]=\sum_i\mathbb{E}\left[X\mid A_i\right]p\left(A_i\right)$$
>条件期望公式（2）：$$\mathbb{E}[X\mid Y=y]=\sum_xxp(X=x\mid Y=y)$$

- **推导：**
我们实行从右向左推的原则：

1. 转化：将$V$带入剔除
$$\mathbb{E}[V(S_{t+1})|S_t]=\mathbb{E}[\mathbb{E}[G_{t+1}|S_{t+1}]|S_t]$$

2. 推导：
证明: 为了符号简洁并且易读,我们去掉下标,令 $S_t=s$ , $G_{t+1}=g'$ ,$S_{t+1}=s'$:
$$\begin{aligned}
\mathbb{E}\left[\mathbb{E}\left[G_{t+1}\mid s_{t+1}\right]\mid s_{t}\right]
&=\mathbb{E}\left[\mathbb{E}\left[g^{\prime}\mid s^{\prime}\right]\mid s\right] \\
&=\mathbb{E}\left[\sum_{g^{\prime}}g^{\prime}\left.p\left(g^{\prime}\mid s^{\prime}\right)\mid s\right]\right.  \\
&=\sum_{s^{\prime}}\sum_{g^{\prime}}g^{\prime}p\left(g^{\prime}\mid s^{\prime},s\right)p\left(s^{\prime}\mid s\right) \\
&=\sum_{s^{\prime}}\sum_{g^{\prime}}\frac{g^{\prime}p\left(g^{\prime}\mid s^{\prime},s\right)p\left(s^{\prime}\mid s\right)p(s)}{p(s)} \\
&=\sum_{s^{\prime}}\sum_{g^{\prime}}\frac{g^{\prime}p\left(g^{\prime}\mid s^{\prime},s\right)p\left(s^{\prime},s\right)}{p(s)} \\
&=\sum_{s^{\prime}}\sum_{g^{\prime}}\frac{g^{\prime}p\left(g^{\prime},s^{\prime},s\right)}{p(s)} \\
&=\sum_{s^{\prime}}\sum_{g^{\prime}}g^{\prime}p\left(g^{\prime},s^{\prime}\mid s\right) \\
&=\sum_{g^{\prime}}\sum_{s^{\prime}}g^{\prime}p\left(g^{\prime},s^{\prime}\mid s\right) \\
&=\sum_{g^{\prime}}g^{\prime}p\left(g^{\prime}\mid s\right) \\
&=\mathbb{E}\left[g^{\prime}\mid s\right]\\
&=\mathbb{E}\left[G_{t+1}\mid s_t\right]
\end{aligned}$$


### V贝尔曼方程的概率公式
状态值函数继续推导：
$$
V (s)= R_s+\gamma\mathbb{E}\left[V(S_{t+1}) \mid S_t = s \right]
$$
$$V(s)={R_s}+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}\mid s)\operatorname{V(s^{\prime})}$$
***
### 求解状态值函数——逆矩阵法

状态值函数：
$$V(s)={R_s}+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}\mid s)\operatorname{V(s^{\prime})}$$
若状态转移矩阵or状态转移函数已知，则可以采用**逆矩阵法**：

我们用矢量公式表示上述式子：
$$v=\mathcal{R}+\gamma\mathcal{P}v$$
其中$v$ 和$r$矢量，其**单元$v(s)$ 和$R(s)$ 是对所有$s\in S$ 的**，而$P$是转移概率矩阵，其元素 $p(s^{\prime}|s)$ 对所有 $s,s^{\prime}\in\mathcal{S}$ 成立。这里的v状态值函数表示所有状态下的值函数。

>第一，这里可以借鉴现代控制理论的方法，你看现代控制理论也都是状态1、状态2，而不是时间步。所以这个状态值函数只和状态有关。

> 第二，状态值函数表示的是当前时间步和下一个时间步s的关系，那么当前时间步可能是$s(x_1)$,下一个时间步的状态可能是$s(x_2)$。矢量公式说的正是状态本身编号，而不是状态时间步的编号。每一次时间步的连接，可能体现在矢量公式上都是乱连的，最后垒起来，构成像神经网络一样的连接结构。于是也就可以用矢量计算。![[Pasted image 20231120001740.png]]


由$v=r+\gamma Pv$,我们可以直接对它求解：



$$\begin{bmatrix}v({s_1})\\\vdots\\v({s_n})\end{bmatrix}=\begin{bmatrix}\mathcal{R}_{s_1}\\\vdots\\\mathcal{R}_{s_n}\end{bmatrix}+\gamma\begin{bmatrix}\mathcal{P}_{11}&\dots&\mathcal{P}_{1n}\\\vdots\\\mathcal{P}_{11}&\dots&\mathcal{P}_{nn}\end{bmatrix}\begin{bmatrix}v({s_1})\\\vdots\\v(s_n)\end{bmatrix}$$

对它进行矩阵求解得：
$$v=(I-\gamma P)^{-1}r$$

逆矩阵方法提供了**一个与时间步无关的整体视角**，通过一次性求解得到整个状态空间上的最优值函数。但是求解的复杂度为$O(n^3)$ ,其中$n$ 是状态的数量，于是它对大规模或者连续问题不适用。后续可以考虑迭代方法，比如动态规划（DP）、蒙特卡洛估计（MC）、时间差分（TD）。

## 3. **马尔可夫决策过程（MDP）:**

马尔可夫决策过程（Markov Decision Process）是马尔可夫奖励过程的一个进一步扩展，引入了决策者的概念。有了决策者，即有了动作。MDP的目标是**找到一个策略$\pi$**，即从状态到动作的映射，**使得长期累积的奖励最大化**。

**在强化学习中，找到一个最优策略就意味着找到一个最佳的概率分布，这个分布描述了在每个状态下选择每个动作的概率。**

序列过程：
$$s_0,a_0,s_1,{r_1},a_1,s_2,{r_2}...s_{t-1},{r_{t-1}},a_{t-1},s_t,{r_t}...,$$

一组序列为：$(s,a,s^{\prime},\color{red}{r})$

![[Pasted image 20231119215558.png]]



   马尔可夫决策过程包含状态空间 $S$、动作空间 $A$、状态转移概率 $P$、奖励函数 $R$ 和折扣因子 $\gamma$。
   - 状态转移概率 $P$：$P(s'| s, a) = \mathbb{P}(S_{t+1}=s' | S_t=s, A_t=a)$表示在执行动作 $a$ 后，从状态 $s$ 转移到状态 $s'$ 的概率。
   - 奖励函数 $R$：表示在执行动作 $a$ 后，从状态 $s$ 转移到状态 $s'$ 可能获得的奖励。
   - 折扣因子 $\gamma$：用于衡量未来奖励的重要性，通常取值在 $0$ 到 $1$ 之间。
   - 策略$\pi$:可以是确定性策略（确定每个状态下选择的动作）或随机策略（在每个状态下选择动作的概率分布）

### 公式更新

**添加策略函数之后，一些公式更新：**
- 状态转移概率$P_{\color{red}{\pi}}(s^{\prime}\mid s^{\prime})$：

$$P_{\color{red}{\pi}}(s^{\prime}\mid s)=\sum_{a\in A}\pi(a\mid s)P(s^{\prime}\mid a,s)$$
- 奖励函数$R_s^{\color{red}{\pi}}$：
$$R_s^{\color{red}{\pi}}=\sum_{a\in A}\pi(a \mid s)R_s^a$$
$$R_s^a = \mathbb{E_\pi}[R_{t+1} | S_t=s,A_t=a]$$

- 价值函数的更正：
$$V^\pi(s)={R_s^\pi}+\gamma\sum_{s\in S^{\prime}}P_\pi(s^{\prime}\mid s)V^\pi(s^{\prime})$$

### 动作值函数Q
由于引入了策略和动作，于是我们可以设定一个新的价值函数，即动作值函数（Action-Value Function）。通过策略 $\pi(a|s)$，表示在状态 $s$ 下选择动作 $a$ 的概率，仿照状态值函数 $V^\pi$ 的定义方法，我们去定义动作值函数 $Q^\pi$：

$$Q^\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]$$



### Q的贝尔曼方程
得到定义后，我们可以去推导它的贝尔曼方程（仿照状态价值函数的推导
$$
Q^\pi(s,a) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s,A_t=a \right]
$$
同理推导得到：
$$
\begin{aligned}
Q^\pi(s,a)&=\mathbb{E}_\pi\left[R_{t+1}+\gamma Q^\pi(S_{t+1},A_{t+1})\mid S_t=s,A_t=a\right]
\\
&=\mathbb{E_\pi}[R_{t+1}\mid S_{t}=s,A_t=a]+\gamma \mathbb{E_\pi}[Q^\pi(S_{t+1},A_{t+1})\mid S_t=s,A_t=a]\\
&=R_s^a+\gamma \mathbb{E_\pi}[Q^\pi(S_{t+1},A_{t+1} )\mid S_t=s,A_t=a]

\end{aligned}

$$
***
### Q贝尔曼方程的概率公式
接下来我们就要运用条件期望公式去展开$\mathbb{E_\pi}[Q^\pi(S_{t+1},A_{t+1}) \mid S_t=s,A_t=a]$ :
我们可以使用条件期望的定义来展开这个表达式。回顾条件期望的定义：



$$\mathbb{E}_\pi\left[Q^\pi(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a\right] $$

对于离散型变量的情况，我们可以使用求和的形式：

$$
\begin{aligned}
\mathbb{E}_\pi\left[Q^\pi(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a\right]
\\&= \sum_{s', a'} Q^\pi(s', a') P(S_{t+1}=s', A_{t+1}=a' \mid S_t=s, A_t=a, \pi) \\
&=\sum_{s', a'} Q^\pi(s', a') P(s', a' \mid s,a) 
\end{aligned}
$$

其中：
$$
\sum_{s', a'}  P(s', a' \mid s,a)
=\sum_{s\in S^{\prime}}P(s^{\prime}\mid a,s)\sum_{a\in A}\pi(a^{\prime}\mid s^{\prime})
$$
可以想象，由于有了状态$s$ 和动作 $a$ 才有了状态 $s'$ .有了状态 $s'$ 才通过策略来选择动作 $a'$。所以可以这样分解。

于是

$$
\mathbb{E}_\pi\left[Q^\pi(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a\right]=\sum_{s\in S^{\prime}}P(s^{\prime}\mid a,s)\sum_{a\in A}\pi(a^{\prime}\mid s^{\prime})Q_\pi(s^{\prime},a^{\prime})
$$

***

最终得到

$$Q^\pi(s,a) =R_s^a+\gamma\sum_{s\in S^{\prime}}P(s^{\prime}\mid a,s)\sum_{a\in A}\pi(a^{\prime}\mid s^{\prime})Q_\pi(s^{\prime},a^{\prime})$$


# 总结
## MDP的名词
1. **状态（State）：** 表示系统或环境可能处于的各种情况或配置。状态集合 $S$ 。
2. **动作（Action）：** 代理或决策者在某个状态下可以执行的操作。动作集合 $A$ 。
3. **状态转移矩阵（State Transition Matrix）：**
   在马尔可夫决策过程（MDP）中，状态转移矩阵是一个描述状态之间转移概率的矩阵。如果有 $N$ 个状态，那么状态转移矩阵 $P$ 的元素 $P_{s,s'}$ 表示在当前处于状态 $s$ 时，下一步转移到状态 $s'$ 的概率。状态转移矩阵通常表示为：
   $$P = \begin{bmatrix} P_{1,1} & P_{1,2} & \ldots & P_{1,N} \\ P_{2,1} & P_{2,2} & \ldots & P_{2,N} \\ \vdots & \vdots & \ddots & \vdots \\ P_{N,1} & P_{N,2} & \ldots & P_{N,N} \end{bmatrix}$$
4. **状态转移概率（Transition Probability）：** 表示在执行某个动作后，系统从一个状态转移到另一个状态的概率。通常用 $P(s' | s, a)$ 表示在状态 $s$ 下执行动作 $a$ 后进入状态 $s'$ 的概率。
5. **奖励函数（Reward Function）：** 描述在执行某个动作后，从一个状态转移到另一个状态可能获得的即时奖励。通常用 $R(s, a, s')$ 表示。
6. **折扣因子（Discount Factor）：** 用符号 $\gamma$ 表示，用于衡量未来奖励的重要性。折扣因子在 $0$ 到 $1$ 之间，越接近 $1$ 表示越重视未来奖励。
7. **回报（Return）：**
   在马尔可夫决策过程中，回报是决策者从起始状态开始，执行一系列动作并遵循某个策略后所获得的累积奖励。回报通常用 $G_t$ 表示，在时间步 $t$ 开始的回报定义为：
   $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
   其中，$R_{t+k+1}$ 是在时间步 $t+k+1$ 处获得的即时奖励，$\gamma$ 是折扣因子。
8. **策略（Policy）：** 表示决策者在每个状态下选择动作的规则或方案。策略通常用符号 $\pi$ 表示，可以是确定性策略（确定每个状态下选择的动作）或随机策略（在每个状态下选择动作的概率分布）。
9. **价值函数（Value Function）：** 衡量状态或状态-动作对的好坏程度，用于指导决策者的行为。有状态值函数 $V(s)$ 和动作值函数 $Q(s, a)$ 两种形式。
   - **状态值函数 $V(s)$：** 表示在策略 $\pi$ 下从状态 $s$ 开始的预期折扣累积奖励。
   - **动作值函数 $Q(s, a)$：** 表示在策略 $\pi$ 下从状态 $s$ 执行动作 $a$ 开始的预期折扣累积奖励。

10. **时间范围（Horizon）：**
   时间范围指的是在一个马尔可夫决策过程中，代理或决策者考虑未来的时间步数。


## V&Q值函数
### 表格总结

  类别|状态值函数$V^\pi(s)$|动作-状态值函数$Q^\pi(s, a)$
-|:-|:-
定义①|$\mathbb{E}_\pi \left[ G_t \mid S_t = s \right]$|$\mathbb{E}_\pi[G_t \mid S_t = s, A_t = a]$
Ballman②|$R_s^\pi+\gamma\mathbb{E_\pi}\left[V^\pi(S_{t+1}) \mid S_t = s \right]$|$R_s^a+\gamma \mathbb{E_\pi}[Q^\pi(S_{t+1},A_{t+1} )\mid S_t=s,A_t=a]$
概率③|${R_s^\pi}+\gamma\sum_{s\in S^{\prime}}P_\pi(s^{\prime}\mid s)V^\pi(s^{\prime})$|$R_s^a+\gamma\sum_{s\in S^{\prime}}P(s^{\prime}\mid a,s)\sum_{a\in A}\pi(a^{\prime}\mid s^{\prime})Q_\pi(s^{\prime},a^{\prime})$

### 两者关系
1. **状态值函数 $V^\pi(s)$ 和动作值函数 $Q^\pi(s, a)$ 之间的关系：**
   $$V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s, a) $$

   这个关系表达了在策略 $\pi$ 下，当前状态 $s$ 的值等于在当前状态下对所有可能动作 $a$ 采取的动作值函数 $Q^\pi(s, a)$ 的加权和，其中权重是按照策略 $\pi$ 选择动作 $a$ 的概率。

2. **动作值函数 $Q^\pi(s, a)$ 和状态值函数 $V^\pi(s)$ 之间的关系：**
   $$Q^\pi(s, a) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_t = s, A_t = a \right] $$

   这个关系表达了在策略 $\pi$ 下，当前状态 $s$ 采取动作 $a$ 的动作值函数等于在执行动作 $a$ 后，根据策略 $\pi$ 转移到下一状态 $S_{t+1}$ 并获得奖励 $R_{t+1}$ 的期望值加上未来折现的状态值函数 $V^\pi(S_{t+1})$。

- **状态值函数 $V^\pi(s)$ 衡量的是在某个状态下策略的期望回报。**
- **动作值函数 $Q^\pi(s, a)$ 衡量的是在某个状态下采取某个动作后的期望回报。

*** 
关系推导如下：

$$
\begin{aligned}
V^\pi(s)&={R_s^\pi}+\gamma\sum_{s\in S^{\prime}}P_\pi(s^{\prime}\mid s)V^\pi(s^{\prime})\\
&=\sum_{a\in A}\pi(a \mid s)R_s^a+\gamma\sum_{s\in S^{\prime}}\sum_{a\in A^{\prime}}\pi(a,s)P(s'\mid a,s)V^\pi(s')\\
&=\sum_{a\in A}\pi(a \mid s)(R^a_s+\gamma\sum_{s\in S^{\prime}}P(s' \mid a,s)V^\pi(s'))\\


\end{aligned}
$$

若假设如下关系成立   $$V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s, a)\tag{1} $$
则
$$
Q^\pi(s,a)=R^a_s+\gamma\sum_{s\in S^{\prime}}P(s' \mid a,s)V^\pi(s')\tag{2}
$$

对比$Q^\pi(s,a)$原公式：
$$Q^\pi(s,a) =R_s^a+\gamma\sum_{s\in S^{\prime}}P(s^{\prime}\mid a,s)\sum_{a\in A}\pi(a^{\prime}\mid s^{\prime})Q_\pi(s^{\prime},a^{\prime})\tag{3}$$
则如下成立：

$$
V^\pi(s')=\sum_{a\in A}\pi(a^{\prime}\mid s^{\prime})Q_\pi(s^{\prime},a^{\prime})\tag{4}
$$

我们将假设的（1）和推理得到的（4）对比，发现前后对应。所以关系成立。


### 贝尔曼最优方程

https://zhuanlan.zhihu.com/p/421406358

理论依据：巴拿赫不动点定理

关键步骤：证明贝尔曼方程为某个度量空间(X,d)上的压缩映射。

由此可以通过最优贝尔曼算子的重复应用最终将导出唯一的最优值函数函数，通过值函数可以得到最优策略。

*** 
什么是巴拿赫不动点定理？
https://zhuanlan.zhihu.com/p/112642861

通俗理解：
想象你有一个神奇的函数，这个函数有一个特别的性质：无论你从哪里开始，用这个函数做一次操作，结果都会距离起点更近。这就好像是在玩一个游戏，你每一步都在朝着目标靠拢。
巴拿赫不动点定理告诉我们，如果我们一直使用这个函数，无论从哪里开始，最终都会到达一个特殊的点，这个点不会再被这个函数推动，因为它就是这个函数的“不动点”。

这个定理在数学中有很多应用，它保证了在一些情况下，我们通过迭代（重复操作）可以找到一个固定的状态。这个定理的核心思想就是通过一系列小步骤，确保我们最终会到达一个特殊的位置。

***

由于MDP的目标是最大化价值函数。于是可以定义最优状态价值函数V为：
$$V_*(s)=\max_\pi V^\pi(s),\forall s\in\mathcal{S},$$

同理也会有最优动作价值函数Q：
$$Q_*(s,a)=\max_{\pi}Q^\pi(s,a),\forall s\in\mathcal{S},a\in\mathcal{A},$$

   $$
\begin{aligned}
Q_*(s, a) &= \mathbb{E}_\pi \left[ R_{t+1} + \gamma V_*(S_{t+1}) \mid S_t = s, A_t = a \right] \\


\end{aligned}
   $$
所以最大化那个都可以直接最大化到另一个。
![[Pasted image 20231117175118.png]]
#### 贝尔曼方程和贝尔曼最优方程的异同

贝尔曼方程是有确定性策略，而贝尔曼最优性方程还要找到最优策略。

- **贝尔曼方程：** 贝尔曼方程描述了在**给定某个策略（可以是确定性策略或随机策略**）的情况下，当前状态值函数与后续状态值函数之间的关系。它是在考虑特定策略的情况下的值函数更新关系。

- **贝尔曼最优性方程：** 贝尔曼最优性方程描述了**在寻找最优策略的情况下**，最优状态值函数与最优后续状态值函数之间的关系。它表示了在**最优策略**下，当前状态值函数与后续状态值函数之间的最优关系。

在贝尔曼最优性方程中，通过最大化操作，我们考虑了在当前状态下选择最优动作的情况，从而获得最优状态值函数或最优动作值函数。这意味着我们在贝尔曼最优性方程中要找到的是最优策略下的值函数。

所以，**可以说贝尔曼方程是在已知策略的情况下进行值函数更新，而贝尔曼最优性方程则是在寻找最优策略的情况下进行值函数更新。**
# 动态规划

动态规划（DP）用于解决具有**重叠子问题**（在解一个问题时，需要多次解相同的子问题。动态规划通过保存已解决的子问题的结果，避免重复计算）和**最优子结构**（如果问题的最优解所包含的子问题的解也是最优的）的问题。它将一个问题分解为一系列相互重叠的子问题，并通过存储和重复计算已解决的子问题的结果来节省计算时间，从而实现高效求解整体问题。
因为马尔可夫决策过程（MDP）具有最优子结构和重叠子问题的性质。所以能够应用于强化学习中。

**总体来说，动态规划方法通过MDP的转移概率和奖励函数的形式对环境进行显式建模，利用MDP的结构和转移动态来寻找最优解。**


***



在强化学习中，动态规划（Dynamic Programming）是一种基于状态值函数或动作值函数的迭代方法，用于求解马尔可夫决策过程（MDP）中的最优策略。



## 迭代
## 策略迭代（Policy Iteration）：

策略迭代是一种结合策略评估和策略改进的方法，通过反复进行策略评估和策略改进的迭代，最终得到最优策略。
### 1. **策略评估（Policy Evaluation）：**

策略评估的目标是计算给定策略下的状态值函数或动作值函数。

- **状态值函数$V^\pi$的更新：**
  $$V^\pi(s) \leftarrow \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma V^\pi(s')]$$
  
- **动作值函数$Q^\pi$ 的更新：**
  $$Q^\pi(s, a) \leftarrow \sum_{s', r} p(s', r | s, a) [r + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a')]$$



### 2. **策略改进（Policy Improvement）：**

策略改进的目标是在已经计算得到的值函数的基础上，进一步改进策略。这一过程基于贪婪策略（Greedy Policy）选择能够最大化值函数的动作。

- **贪婪策略 $\pi'(s) = \text{argmax}_a Q^\pi(s, a)$**
  
如果我们在某个状态下总是选择最大值的动作，可能会陷入局部最优，因为这个动作可能只是在当前策略下是最好的，但在更新策略后可能并不是最好的。

[[3. 预测和控制#广义策略迭代]]
![[Pasted image 20231115191135.png]]

![[Pasted image 20231121232320.png]]
## 值迭代（Value Iteration）

值迭代是一种单步同时更新所有状态的值函数的方法，通过迭代计算值函数，最终得到最优值函数，从而得到最优策略。

当我们使用值迭代找到最优值函数后，最优策略的提取是一个额外的步骤，因为值迭代的主要目标是找到每个状态的最优值，而不是直接找到最优策略。以下是这个过程的更详细的解释：


1. **值函数找到：** 在值迭代的过程中，我们通过贝尔曼最优方程迭代更新每个状态的值函数，直到值函数收敛。这时，我们得到了一个最优值函数，记为 $V^*$。
$$ V(s) \leftarrow \max_a \left[ \sum_{s'} P(s' \mid s, a) \cdot \left( R(s, a, s') + \gamma \cdot V(s') \right) \right] $$

2. **策略提取：** 一旦最优值函数 $V^*$ 稳定，我们可以从中提取最优策略。这意味着对于每个状态，我们选择使值函数最大化的动作，即：

   $$\pi^*(s) = \arg\max_a \left[ \sum_{s'} P(s' \mid s, a) \cdot \left( R(s, a, s') + \gamma \cdot V^*(s') \right) \right] $$


3. **动作选择：** 最终，通过这个过程，我们得到了在每个状态下使值函数最大化的动作，形成了最优策略。这个策略告诉我们在每个状态下应该采取什么样的动作，以达到最大的累积奖励。

**总的来说，值迭代通过迭代更新值函数找到最优值函数，然后通过额外的步骤从最优值函数中提取最优策略。这相当于在值函数更新的时候只是保留了值，然后选出了最大的值作为新的值函数，这时候并不能索引到这个值函数对应的策略。**

![[Pasted image 20231115191354.png]]
![[Pasted image 20231121232733.png]]
## 总结

1. **策略评估（Policy Evaluation）：**
   - **目标：** 计算**给定策略**下的状态值函数（$V^\pi$）或动作值函数（$Q^\pi$）。
   - **方法：** 通过迭代或解线性方程组等方式，更新值函数，直到收敛为止。

2. **策略改进（Policy Improvement）：**
   - **目标：** 在已经**计算得到的值函数**的基础上，进一步改进策略。
   - **方法：** 贪婪地选择能够最大化值函数的动作，形成**新的策略**。

3. **策略迭代（Policy Iteration）：**
   - **方法：** 交替进行策略评估和策略改进，直到策略不再改变。策略迭代保证最终收敛到最优策略和最优值函数。
   - **步骤：**
      1. 初始策略 $\pi_0$.
      2. 迭代进行策略评估，计算 $V^{\pi_i}$.
      3. 迭代进行策略改进，得到新的策略 $\pi_{i+1}$.
      4. 重复步骤2和步骤3，直到策略不再改变。

4. **值迭代（Value Iteration）：**
   - **目标：** 直接计算最优值函数（$V^*$或$Q^*$）。
   - **方法：** 通过迭代更新值函数，直到收敛。值迭代的每一步都是通过**贝尔曼最优方程**进行更新。
   - **步骤：**
      1. 初始化值函数 $V_0$.
      2. 迭代进行值函数更新，计算 $V_{i+1}$.
      3. 重复步骤2，直到 $V_i$ 收敛。



# 总结
在学习任务中，我已经学习了有关强化学习的基本概念，特别是关于强化学习中两个最为重要的函数：状态价值函数和动作价值函数。由于强化学习的任务是解决序列决策问题，通常将其简化为马尔可夫决策问题（MDP）。由于MDP具有重叠子问题和最优子结构的性质，我们能够推导出这两个值函数的贝尔曼公式。有了这些贝尔曼公式，我们就能够进行展开更新，并随后使用动态规划（DP）进行更新。在动态规划的框架下，由于贝尔曼公式的存在，我们引入了值迭代和策略迭代两种方法。在策略迭代中，我们使用了策略评估和策略改进的步骤，相比之下，与只使用贝尔曼公式进行值迭代相比，策略迭代的方式更为高效。

然而，这种基于动态规划的方法在面对大规模问题时并不太适用。如果问题规模较大，可以考虑使用近似学习或深度学习的方法。



