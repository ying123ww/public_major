DQN：深度Q网络算法。

Q-learning求解办法：
1. 表格求解 or线性函数逼近：Q-learning可以收敛于最优解。
2. 非线性函数（神经网络）逼近，Q-learning并不稳定。
3. DQN解决上述问题。

# 背景回顾
无模型方式是指在没有对MDP转移概率分布或者回报函数建模的情况下，再去找最优策略去得到最优价值函数。求解方法有TD（时间差分）。通过自举的方式来估计子问题的值。
由前面证得，在策略$\pi$下，状态$s$ 的状态值函数$V_\pi$：
$$
V^\pi(s)=\mathbb{E_\pi}[R_t+\gamma V^\pi(S_{t+1}) \mid S_t=s]
$$
那么TD学习用自举的方式去分解了上述估计。我们采用一步自举TD(0)：
$$
V(S_t)\leftarrow V(S_t)+ \alpha[R_t +\gamma V(S_{t+1})-V(S_t)]
$$
此时$R_t+\gamma V(S_{t+1})$ 被称为TD目标，$R_t +\gamma V(S_{t+1})-V(S_t)$ 被称为TD误差。
我们可以想象，通过这样的迭代，可以不断逼近TD目标。

## Q-learning
$$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma\max_aQ(s_{t+1},a)-Q(s_t,a_t)]$$

最主要的三步骤:
1. 通过$\epsilon-$ 贪心来选择$A_t$
2. 通过和环境的交互得到下一个状态和奖励
3. 基于奖励和先前的去更新Q。
![[Pasted image 20231124144120.png]]

那么再区别一下Q-learning和Sarsa。
	Q-learning的行为策略是$\epsilon$-贪婪策略，目标策略是贪心策略
	Sarsa:行为策略： 都是贪心策略。
# DQN
## 价值函数逼近
deep Q learning究竟是什么？
因为Q就是由动作s和状态a决定的嘛，输出就是Q值(一个数)。但对于deep Q learning来说呢，输入的是状态s，输出的是这个s的一个Q值向量，每个索引对应不同动作。所以就是拟合这一段Q（s）。因为拟合的话，所以肯定还有参数，所以参数就定为$\theta$.



![[Pasted image 20231126222107.png]]
我们首先要了解，Q-learning的最初的求解方法是Q表格。但是非常低效。
可以对比一下，如果是Q表的话，高维空间上的点，就要表示为一个状态（虽然感觉直觉也是这样(lll￢ω￢)，但是实际上是非常低能的，那样的Q表会超级复杂，而且Q表能处理的是离散数据，更不用说连续数据了。所以如果是神经网络的话，我们可以把一个维度作为一个状态，而且神经网络也容易处理连续输入数据，所以对于输入数据这一块来说，神经网络完胜！
总结：**在 Q表中我们描述状态空间的时候一般用的是状态个数，而在神经网络中我们用的是状态维度**。
那什么是输出？输出都是每个动作对应的Q值，即预测。如果想要输出动作，要用结合贪心算法选择 
Q值最大对应的动作这样。
但是神经网络的缺点：输入可以连续，但是输出只能离散，这个只能用后续的策略梯度方法。

![[Pasted image 20231124152501.png]]


我们考虑Q-learning中使用神经网络（参数$\theta$，输入每个状态为维度，输出动作对应的Q）为去进行函数拟合。

## 例子引入
![[Pasted image 20231126223931.png]]
输入： 4 帧堆栈作为一个状态$s$ 
输出：每个可能的动作输出 Q 值向量，即Q向量，每个位置是不同的动作。
然后，就像 Q-Learning 一样，我们只需要使用 epsilon-greedy 策略来选择要采取的操作。（即一定概率随机选择动作，一定概率选择向量Q中Q值最大的对应的动作。）

> 预处理
> 灰度化，裁剪页面等。减少输入容量。

>为什么四帧作为一个状态？
>需要时间信息。增加运动感！
## 定义

定义：
 Deep Q-Learning uses a deep neural network to approximate the different Q-values for each possible action at a state (value-function estimation)使用深度神经网络来近似某个状态下每个可能动作的不同 Q 值。即输入s，输出Q向量（每个位置对应不同动作的Q值）

- 和Q-learning的不同
Q-learning：要用这个函数取更新Q表，使得Q表中的值更加逼近真值。
$$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma\max_aQ(s_{t+1},a)-Q(s_t,a_t)]$$

而DQN，更新这个神经网络。使得Q网络拟合的更好。
那么如何更新呢，见下文的参数更新。

## 参数更新

因为要Q网络拟合的更好，所以我们需要减少目标和本身的差距，这时候就会有一个本体Q和一个目标Q（Q-Target）。然后我们尽可能减少目标和本体的差值（即Q-Loss），所以我们要使得Q-Loss最小。
![[Pasted image 20231126225208.png]]
总结来说的公式即：
$$
\theta_t\leftarrow\arg\min_\theta\mathcal{L}(Q(S_t,A_t;\theta),R_t+\gamma Q(S_{t+1},A_{t+1};\theta))
$$
这里的更新可以用梯度下降法来更新。
此处的$\mathcal{L}$ 代表损失函数，如均方误差。

## DQN算法

- Sampling: we perform actions and store the observed experience tuples in a replay memory.
	采样：我们执行动作并将观察到的经验元组存储在replay memory中。
- Training: Select a small batch of tuples randomly and learn from this batch using a gradient descent update step.
	训练：随机选择一小批元组（from replay memory），并使用梯度下降更新步骤从该批次中学习。

![[Pasted image 20231126225945.png]]

但会有不稳定问题的出现：
1. 由于非线性Q值函数的出现。（神经网络）
2. 由于 bootstrapping，就是TD采用的这个。用现有估计而不是实际的完整回报去更新目标。

DQN通过**两个关键技术**结合Q-learning和深度学习来解决了不稳定的问题。


## 两个关键技术
### 经验回放replay memory
总结：在每个时间步$t$,中，DQN先把智能体获得的经验$(S_t,A_t,R_t,S_{t+1})$ 存入回放缓存中，然后从该缓存中均匀采样小批量样本用于Q-learning的更新。
好处有二：
1. 增加样本效率 
	在训练期间重复使用的经验样本。
2. 减少样本之间的相关性。

In the Deep Q-Learning pseudocode, we initialize a replay memory buffer D with capacity N (N is a hyperparameter that you can define). We then store experiences in the memory and sample a batch of experiences to feed the Deep Q-Network during the training phase.我们初始化容量为 N 的重播内存缓冲区 D（N 是您可以定义的超参数）。然后，我们将经验存储在内存中，并在训练阶段对一批经验进行采样以提供给深度 Q 网络。
![[Pasted image 20231126230647.png]]
***
以下来自joyRL的说法，我觉得说的挺好，于是借鉴于此促进理解：

我们知道一个状态转移就是$(S_t,A_t,R_t,S_{t+1})$ 。在 Q-learning算法中，每次交互得到一个样本之后，就立马拿去更新模型了。但是对于神经网络来说，就会产生一些问题，这跟梯度下降有关。首先每次用单个样本去迭代网络参数很容易导致训练的不稳定，从而影响模型的收敛，在深度学习基础的章节中我们也讲过小批量梯度下降是目前比较成熟的方式。其次，每次迭代的样本都是从环境中实时交互得到的，这样的样本是有关联的，而梯度下降法是基于一个假设的，即训练集中的样本是独立同分布的。

在深度学习中其实是没有这样的问题的。因为训练集是事先准备好的，每次迭代的样本都是从训练集中随机抽取的，因此每次迭代的样本都是独立同分布的。但是这样的假设在强化学习中是不成立的，因为每次迭代的样本都是从环境中实时交互得到的，因此每次迭代的样本都是相互关联的。

换句话说，直接将 Q-learning算法训练的方式来更新 DQN的模型相当于是最原始的梯度下降方式，距离目前最成熟的小批量梯度下降方式还有一定的差距，因此我们需要进行一些处理来达到相同的效果，这就是经验回放的实现初衷。

![[Pasted image 20231124154933.png]]

经验回放的容量是需要有一定的容量限制的，不能太小，也不能太大。
	本质上是因为在深度学习中我们拿到的样本都是事先准备好的，即都是很好的样本，但是在强化学习中样本是由智能体生成的，在训练初期智能体生成的样本虽然能够帮助它朝着更好的方向收敛，但是在训练后期这些前期产生的样本相对来说质量就不是很好了，此时把这些样本喂入智能体的深度网络中更新反而影响其稳定。这就好比我们在小学时积累到的经验，会随着我们逐渐长大之后很有可能就变得不是很适用了，所以经验回放的容量不能太小，太小了会导致收集到的样本具有一定的局限性，也不能太大，太大了会失去经验本身的意义。





***
### 目标网络Q-Target

#### 问题

TD error=TD target (Q-Target) - the current Q-value (estimation of Q)
 TD 目标=在该状态下采取该操作的奖励加上下一个状态的折扣最高 Q 值。
 但是如果用相同的参数（权重）来估计 TD 目标和 Q 值。那么TD 目标与我们正在更改的参数之间存在显着相关性。
![[Pasted image 20231126235324.png]]
这导致了奇怪的追逐路径（训练中的显着振荡）。
#### 解决办法
- 使用具有固定参数的单独网络来估计 TD 目标。
- 每 C 个步骤从我们的深度 Q 网络复制参数以更新目标网络。

![[Pasted image 20231126235410.png]]






![[Pasted image 20231124155116.png]]

目标网络主要是用于计算前后Q值产生的损失函数。而不是直接用n步和n+1步直接去更新网络，不然太动荡了。
如果当前有个小批量样本导致模型对 Q值进行了较差的过估计，如果接下来从经验回放中提取到的样本正好连续几个都这样的，很有可能导致 Q值的发散，所以目标网络作为独立网络来替代所需Q网络来生成Q-learning的目标，进一步提升网络的稳定性。
目标网络每C步将通过直接复制（硬更新）或者指数衰减军均匀（软更新）来与主Q网络同步。
由于目标网络用的是旧网络，所以目标值的产生不会受到当前最新参数的影响，于是大大减少发散或者震荡的情况。
### 总伪代码
![[Pasted image 20231124150734.png]]
## DQN代码实例
以下参考了joyRL所使用的实例代码，感谢joyRL！

### 定义模型
多层MLP
```python
import torch.nn as nn
import torch.nn.functional as F
class MLP(nn.Module):
    def __init__(self, state_dim,action_dim,hidden_dim=128):
        """ 初始化q网络，为全连接网络
        """
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim) # 输入层
        self.fc2 = nn.Linear(hidden_dim,hidden_dim) # 隐藏层
        self.fc3 = nn.Linear(hidden_dim, action_dim) # 输出层
        
    def forward(self, x):
        # 各层对应的激活函数
        x = F.relu(self.fc1(x)) 
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```

### 经验回放

两个功能：
	push，即将一个transition样本按顺序放到经验回放中，如果满了就把最开始放进去的样本挤掉，因此如果大家学过数据结构的话推荐用队列来写，虽然这里不是。
	sample，很简单就是随机采样出一个或者若干个（具体多少就是batch_size了）样本供DQN网络更新。

```python
from collections import deque
import random
class ReplayBuffer(object):
    def __init__(self, capacity: int) -> None:
        self.capacity = capacity
        self.buffer = deque(maxlen=self.capacity)
    def push(self,transitions):
        ''' 存储transition到经验回放中
        '''
        self.buffer.append(transitions)
    def sample(self, batch_size: int, sequential: bool = False):
        if batch_size > len(self.buffer): # 如果批量大小大于经验回放的容量，则取经验回放的容量
            batch_size = len(self.buffer)
        if sequential: # 顺序采样
            rand = random.randint(0, len(self.buffer) - batch_size)
            batch = [self.buffer[i] for i in range(rand, rand + batch_size)]
            return zip(*batch)
        else: # 随机采样
            batch = random.sample(self.buffer, batch_size)
            return zip(*batch)
    def clear(self):
        ''' 清空经验回放
        '''
        self.buffer.clear()
    def __len__(self):
        ''' 返回当前存储的量
        '''
        return len(self.buffer)
```


### DQN更新过程~芜湖

```python
import torch
import torch.optim as optim
import math
import numpy as np
class DQN:
    def __init__(self,model,memory,cfg):

        self.action_dim = cfg.action_dim  
        self.device = torch.device(cfg.device) 
        self.gamma = cfg.gamma # 奖励的折扣因子
        # e-greedy策略相关参数
        self.sample_count = 0  # 用于epsilon的衰减计数
        self.epsilon_start = cfg.epsilon_start
        self.epsilon_end =cfg.epsilon_end
        self.epsilon_decay = cfg.epsilon_decay
        self.batch_size = cfg.batch_size
        self.policy_net = model.to(self.device)
        self.target_net = model.to(self.device)
        # 复制参数到目标网络
        for target_param, param in zip(self.target_net.parameters(),self.policy_net.parameters()): 
            target_param.data.copy_(param.data)
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=cfg.lr) # 优化器
        self.memory = memory # 经验回放
    def sample_action(self, state):
        ''' 采样动作
        '''
        self.sample_count += 1
        # epsilon指数衰减
        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \
            math.exp(-1. * self.sample_count / self.epsilon_decay) 
        if random.random() > self.epsilon:
            with torch.no_grad():
                state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)
                q_values = self.policy_net(state)
                action = q_values.max(1)[1].item() # choose action corresponding to the maximum q value
        else:
            action = random.randrange(self.action_dim)
        return action
    @torch.no_grad() # 不计算梯度，该装饰器效果等同于with torch.no_grad()：
    def predict_action(self, state):
        ''' 预测动作
        '''
        state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)
        q_values = self.policy_net(state)
        action = q_values.max(1)[1].item() # choose action corresponding to the maximum q value
        return action
    def update(self):
        if len(self.memory) < self.batch_size: # 当经验回放中不满足一个批量时，不更新策略
            return
        # 从经验回放中随机采样一个批量的转移(transition)
        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(
            self.batch_size)
        # 将数据转换为tensor
        state_batch = torch.tensor(np.array(state_batch), device=self.device, dtype=torch.float)
        action_batch = torch.tensor(action_batch, device=self.device).unsqueeze(1)  
        reward_batch = torch.tensor(reward_batch, device=self.device, dtype=torch.float)  
        next_state_batch = torch.tensor(np.array(next_state_batch), device=self.device, dtype=torch.float)
        done_batch = torch.tensor(np.float32(done_batch), device=self.device)
        q_values = self.policy_net(state_batch).gather(dim=1, index=action_batch) # 计算当前状态(s_t,a)对应的Q(s_t, a)
        next_q_values = self.target_net(next_state_batch).max(1)[0].detach() # 计算下一时刻的状态(s_t_,a)对应的Q值
        # 计算期望的Q值，对于终止状态，此时done_batch[0]=1, 对应的expected_q_value等于reward
        expected_q_values = reward_batch + self.gamma * next_q_values * (1-done_batch)
        loss = nn.MSELoss()(q_values, expected_q_values.unsqueeze(1))  # 计算均方根损失
        # 优化更新模型
        self.optimizer.zero_grad()  
        loss.backward()
        # clip防止梯度爆炸
        for param in self.policy_net.parameters():  
            param.grad.data.clamp_(-1, 1)
        self.optimizer.step() 
```

### 定义训练和测试

```python
def train(cfg, env, agent):
    ''' 训练
    '''
    print("开始训练！")
    rewards = []  # 记录所有回合的奖励
    steps = []
    for i_ep in range(cfg.train_eps):
        ep_reward = 0  # 记录一回合内的奖励
        ep_step = 0
        state, info = env.reset(seed = cfg.seed)  # 重置环境，返回初始状态
        for _ in range(cfg.max_steps):
            ep_step += 1
            action = agent.sample_action(state)  # 选择动作
            next_state, reward, terminated, truncated , info = env.step(action)  # 更新环境，返回transition
            agent.memory.push((state, action, reward, next_state, terminated))  # 保存transition
            state = next_state  # 更新下一个状态
            agent.update()  # 更新智能体
            ep_reward += reward  # 累加奖励
            if terminated:
                break
        if (i_ep + 1) % cfg.target_update == 0:  # 智能体目标网络更新
            agent.target_net.load_state_dict(agent.policy_net.state_dict())
        steps.append(ep_step)
        rewards.append(ep_reward)
        if (i_ep + 1) % 10 == 0:
            print(f"回合：{i_ep+1}/{cfg.train_eps}，奖励：{ep_reward:.2f}，Epislon：{agent.epsilon:.3f}")
    print("完成训练！")
    env.close()
    return {'rewards':rewards}

def test(cfg, env, agent):
    print("开始测试！")
    rewards = []  # 记录所有回合的奖励
    steps = []
    for i_ep in range(cfg.test_eps):
        ep_reward = 0  # 记录一回合内的奖励
        ep_step = 0
        state, info = env.reset(seed = cfg.seed)  # 重置环境，返回初始状态
        for _ in range(cfg.max_steps):
            ep_step+=1
            action = agent.predict_action(state)  # 选择动作
            next_state, reward, terminated, truncated , info = env.step(action)  # 更新环境，返回transition
            state = next_state  # 更新下一个状态
            ep_reward += reward  # 累加奖励
            if terminated:
                break
        steps.append(ep_step)
        rewards.append(ep_reward)
        print(f"回合：{i_ep+1}/{cfg.test_eps}，奖励：{ep_reward:.2f}")
    print("完成测试")
    env.close()
    return {'rewards':rewards}
```


### 定义环境
```python
import gymnasium as gym
import os
def all_seed(seed = 1):
    ''' 万能的seed函数
    '''
    if seed == 0: # 不设置seed
        return 
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed) # config for CPU
    torch.cuda.manual_seed(seed) # config for GPU
    os.environ['PYTHONHASHSEED'] = str(seed) # config for python scripts
    # config for cudnn
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.enabled = False
def env_agent_config(cfg):
    env = gym.make(cfg.env_id) # 创建环境
    all_seed(seed=cfg.seed)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    print(f"状态空间维度：{state_dim}，动作空间维度：{action_dim}")
    setattr(cfg,"state_dim",state_dim) # 更新state_dim到cfg参数中
    setattr(cfg,"action_dim",action_dim) # 更新action_dim到cfg参数中
    model = MLP(state_dim, action_dim, hidden_dim = cfg.hidden_dim) # 创建模型
    memory = ReplayBuffer(cfg.memory_capacity)
    agent = DQN(model,memory,cfg)
    return env,agent
```


### 定义参数

```python
import argparse
import matplotlib.pyplot as plt
import seaborn as sns

class Config:
    def __init__(self) -> None:
        self.algo_name = 'DQN' # 算法名称
        self.env_id = 'CartPole-v1' # 环境id
        self.seed = 1 # 随机种子，便于复现，0表示不设置
        self.train_eps = 100 # 训练的回合数
        self.test_eps = 20 # 测试的回合数
        self.max_steps = 200 # 每个回合的最大步数，超过该数则游戏强制终止
        self.gamma = 0.95 # 折扣因子
        self.epsilon_start = 0.95 # e-greedy策略中初始epsilon
        self.epsilon_end = 0.01 # e-greedy策略中的终止epsilon
        self.epsilon_decay = 500 # e-greedy策略中epsilon的衰减率
        self.memory_capacity = 100000 # 经验回放池的容量
        self.hidden_dim = 256 # 神经网络的隐藏层维度
        self.batch_size = 64 # 批次大小
        self.target_update = 4 # 目标网络的更新频率
        self.lr = 0.0001 # 学习率
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # 检测gpu
        
def smooth(data, weight=0.9):  
    '''用于平滑曲线，类似于Tensorboard中的smooth曲线
    '''
    last = data[0] 
    smoothed = []
    for point in data:
        smoothed_val = last * weight + (1 - weight) * point  # 计算平滑值
        smoothed.append(smoothed_val)                    
        last = smoothed_val                                
    return smoothed

def plot_rewards(rewards,cfg, tag='train'):
    ''' 画图
    '''
    sns.set()
    plt.figure()  # 创建一个图形实例，方便同时多画几个图
    plt.title(f"{tag}ing curve on {cfg.device} of {cfg.algo_name} for {cfg.env_id}")
    plt.xlabel('episodes')
    plt.plot(rewards, label='rewards')
    plt.plot(smooth(rewards), label='smoothed')
    plt.legend()
    plt.show()

```

### 训练与测试
```python
# 获取参数
cfg = Config() 
# 训练
env, agent = env_agent_config(cfg)
res_dic = train(cfg, env, agent)
 
plot_rewards(res_dic['rewards'], cfg, tag="train")  
# 测试
res_dic = test(cfg, env, agent)
plot_rewards(res_dic['rewards'], cfg, tag="test")  # 画出结果
```

# DQN的变种
## double DQN
我们可以看到DQN的Target是：
$$y_j=r_j+\gamma\max_{a^{\prime}}\hat{Q}\Big(\phi_{j+1},a^{\prime};\theta^{-}\Big)$$
公式中，$\hat{Q}$ , 代表目标网络 ， $\theta^{-}$ 代表目标网络的参数。所以我们可以看到，对于DQN， $\theta^{-}$ 又用于估计$\hat{Q}$ 值，又用于估计下个动作a进行选择。

double DQN的核心思想就是说在这两个阶段使用不同的网络去除选择和评价中噪声的相关性。

Double DQN: Method to handle overestimation of Q-Values. This solution uses two networks to decouple the action selection from the target Value generation。处理 Q 值过估计的方法。该解决方案使用两个网络将操作选择与目标值生成解耦：

- DQN Network to select the best action to take for the next state (the action with the highest Q-Value)DQN 网络去选择下一个状态要采取的最佳操作（具有最高 Q 值的操作）
- Target Network to calculate the target Q-Value of taking that action at the next state. 目标网络计算在下一个状态采取该操作的目标 Q 值。

即
$$y_j=r_j+\gamma \hat{Q}(\phi_{j+1},\underset{a^{\prime}}{\operatorname*{argmax}}Q(\phi_{j+1},a^{\prime};\theta);\theta^{-})$$

