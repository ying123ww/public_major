DDPG是一种结合了**深度学习和强化学习**的算法，用于处理具有**连续动作空间**（离线策略）的问题。它是基于DPG（Deterministic Policy Gradient）算法的扩展，由 Silver 等人在 2014 年提出，后来 Lillicrap 等人于 2016 年通过引入深度学习技术对其进行了改进，从而形成了DDPG。

DDPG的主要特点包括：

1. **演员-评论家（Actor-Critic）框架**：DDPG采用了演员-评论家框架。其中，“演员”（Actor）负责生成动作，它学习一个策略函数，映射状态到一个具体的动作。“评论家”（Critic）则评估这个动作的好坏，它学习一个价值函数，用于评估当前策略下的动作的期望回报。

2. **确定性策略**：不同于传统的强化学习算法使用的随机策略，DDPG使用**确定性策略，直接映射状态到一个具体的动作**。这种方法特别适合于连续动作空间的问题。

3. **深度学习**：DDPG利用**深度神经网络来近似演员和评论家的函数**。这使得算法能够处理高维的状态空间和复杂的环境。

4. **经验回放（Experience Replay）**：DDPG存储过去的转换（状态、动作、奖励等）在一个回放缓冲区中，并从中随机抽取小批量的转换来更新网络。这有助于打破数据间的相关性，同时重复利用过去的经验。

5. **目标网络（Target Networks）**：DDPG使用了两组网络，**一组用于预测（演员和评论家），另一组作为目标网络**。目标网络的参数会缓慢更新，以稳定学习过程。




