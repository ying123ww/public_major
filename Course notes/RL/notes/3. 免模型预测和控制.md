对于什么是免模型[[1.基础定义+介绍#两大模型——有模型和免模型]]
总之就是：在免模型强化学习中，智能体只关心从当前状态到目标状态的最佳路径，而不考虑对环境动态的建模。即智能体通过不断尝试不同的动作，观察奖励，然后更新Q值（动作值函数），从而逐步学习最佳策略。这不需要事先了解环境的转移概率或奖励函数。
# 免模型预测
关于预测：[[1.基础定义+介绍#预测（Prediction）：]]。
总之，预测就是为了估计值函数。
![[Pasted image 20231119202422.png]]
## 蒙特卡洛预测（MC）
这是一种用于估计**状态值函数**的免模型强化学习方法。
总结为多次求平均（相当于求期望）。
**learning at the end of the episode**
### 基本步骤
以下是蒙特卡洛预测的基本思想和步骤：
1. **问题定义：**
考虑一个马尔可夫决策过程（MDP），智能体与环境进行交互，产生一个轨迹（trajectory），其中包含一系列状态、动作和奖励。我们的目标是估计每个状态的值函数 $V(s)$。

2. **样本生成：**
智能体与环境交互，生成多个样本轨迹。每个轨迹包含一系列状态、动作和奖励。

3. **计算回报：**
对于每个样本轨迹，计算从每个状态开始的累积回报（cumulative return）。回报是从当前时刻起，智能体获得的所有未来奖励的总和。通常使用折扣因子 $\gamma$ 来考虑未来奖励的衰减。

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots + \gamma^{T-t-1} R_T$$

其中，$G_t$ 是从时刻 $t$ 开始的回报， $R_i$ 是在时刻 $i$ 获得的即时奖励， $T$ 是轨迹的结束时刻。

4. **估计状态值：**
对于每个状态 $s$，计算所有以该状态为起点的回报的平均值，即状态值函数 $V(s)$ 的估计：

$$V(s) \approx \frac{1}{N} \sum_{i=1}^{N} G_{t_i}$$

其中，$N$ 是以状态 $s$ 为起点的轨迹数量， $t_i$ 是在轨迹中**首次访问状态 $s$ 的时刻**。(所以在这里计算的是首次访问蒙特卡洛方法)

这样，通过多次轨迹的样本，我们可以估计每个状态的值函数。
![[Pasted image 20231117164935.png]]


### 蒙特卡洛预测的特点：

- **优点：** 蒙特卡洛方法不需要对环境的动态进行建模，只需要从经验中学习。因此，它在处理未知环境和复杂动态的情况下具有一定的优势。

- **缺点：** 由于蒙特卡洛方法需要等到整个轨迹结束后才能进行值函数的更新，因此在实时学习或在线学习方面可能不如时序差分方法。

### （改进）增量蒙特卡洛

变更更新形式。

这里涉及到一个知识点，将经验均值转化为增量均值。
***
#### 经验均值➡增量均值
- 经验均值（empirical mean）是指一组观测值的平均值，通常用于估计样本的中心趋势。

- 增量均值（incremental mean）是指逐步计算均值的过程，而不是等到所有样本都出现后再计算整体均值。这在强化学习中的增量式学习中经常使用，特别是在与环境的交互中逐步更新值函数的情况下。

将经验均值转换成增量均值意味着在每次有新的样本出现时，通过更新当前均值而不是重新计算整体均值。
数学上，假设有一系列观测值 $x_1, x_2, \ldots, x_n$，经验均值为：

$$\text{Empirical Mean} = \frac{1}{n} \sum_{j=1}^{n} x_j$$

推导过程如下：
假设有样本$x_1,x_2,\cdots,x_n$：
$$\begin{aligned}
M_{n}& =\frac1n\sum_{j=1}^nx_j  \\
&=\frac1n\left(x_n+\sum_{j=1}^{n-1}x_j\right) \\
&=\frac1n\left(x_n+(n-1)M_{n-1}\right) \\
&=\frac1n\left(x_t+nM_{n-1}-M_{n-1}\right) \\
&=M_{n-1}+\frac1n\left(x_n-M_{n-1}\right)
\end{aligned}$$

其中，$x_t-M_{n-1}$是残差，$\frac1n$ 类似于学习率 (learning rate) 。当我们得到 $x_n$时，就可以用上一时刻的值来更新现在的值。


所以最后：递推式的增量均值更新规则可以表示为：

$$M_{n} = M_{n-1} + \frac{x_n - M_{n-1}}{n}$$

这样的递推式允许在每次有新的样本出现时，通过简单的更新计算当前均值，而不需要存储所有的观测值。

***

将蒙特卡洛方法更新的方法写成增量式蒙特卡洛 (incremental MC) 方法。我们采集数据，得到一个新的轨迹$(s_1,a_1,r_1,\ldots,s_t)$。对于这个轨迹，我们采用增量的方法进行更新：

$$
\begin{array}{l}N\left(s_t\right)\leftarrow N\left(s_t\right)+1\\V\left(s_t\right)\leftarrow V\left(s_t\right)+\frac1{N\left(s_t\right)}\left(G_t-V\left(s_t\right)\right)\end{array}
$$

我们可以直接把 $\frac1{N(s_t)}$ 变成 $\alpha$ (学习率) ,即

$$
V\left(s_t\right)\leftarrow V\left(s_t\right)+\alpha\left(G_t-V\left(s_t\right)\right)
$$

其中，α代表更新的速率，我们可以对其进行设置。
（这里只有经历了完整的一回合，才能得到累积回报$G_t$，所以说为什么MC方法是learning at the end of the episode）
#### 对比增量蒙特卡洛和DP

增量蒙特卡洛用增量均值，用类似梯度下降法去迭代。
动态规划方法使用贝尔曼公式递推迭代。

- 动态规划方法使用贝尔曼公式, 通过上一时刻的值 $V_{i-1}(s^{\prime})$来更新当前时刻的值$V_i(s)$：
$$V_i(s)\leftarrow\sum_{a\in A}\pi(a\mid s)\left(R(s,a)+\gamma\sum_{s^{\prime}\in S}P\left(s^{\prime}\mid s,a\right)V_{i-1}\left(s^{\prime}\right)\right)$$
![[Pasted image 20231117205647.png]]
- 增量蒙特卡洛：
$$
V\left(s_t\right)\leftarrow V\left(s_t\right)+\alpha\left(G_t-V\left(s_t\right)\right)
$$
![[Pasted image 20231117205624.png]]


所以从图上也可以看出，蒙特卡洛只需要更新一条线上的，而DP需要更新所有状态。

增量蒙特卡洛和动态规划（Dynamic Programming, DP）都是用于估计值函数的方法，但它们有一些关键的区别。下面是它们之间的一些对比：

1. 数据更新方式：
- **增量蒙特卡洛：** 使用逐步更新的方式。在每个时间步，根据新的样本数据进行估计值的更新，例如通过Temporal Difference (TD)学习方法。
- **动态规划：** 通过对整个问题的递归求解来更新值函数。DP方法通常涉及到对值函数的迭代求解，通过Bellman方程的递归形式进行更新。

2. 数据需求：
- **增量蒙特卡洛：** 可以实时地从与环境的交互中获得样本，不需要保存整个轨迹或模型。
- **动态规划：** 需要知道环境的完整动态信息，包括状态转移概率和即时奖励。通常需要事先获得MDP的完整知识。

3. 适用性：
- **增量蒙特卡洛：** 更适用于实时学习、在线学习，以及处理连续数据流的情况。由于其逐步更新的特性，增量蒙特卡洛方法对于大规模问题的处理更为灵活。
- **动态规划：** 更适用于静态环境、已知模型的情况。DP方法通常在计算复杂度上更高，需要对整个状态空间进行迭代，因此在规模较小且问题结构已知的情况下表现更好。

4. 求解最优策略：
- **增量蒙特卡洛：** 增量蒙特卡洛方法本身更注重于值函数的估计，可以通过估计的值函数进一步得到最优策略。
- **动态规划：** DP方法不仅可以估计值函数，还可以直接求解最优策略。通过迭代求解Bellman方程，DP方法可以得到最优值函数和最优策略。


## 时序差分方法（TD）

**learning at each step。**
一个演示图：
https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html

时序差分（Temporal Difference, TD）思想是一种在强化学习中用于**值函数学习**的方法。它结合了动态规划和蒙特卡洛方法的优点，通过不断地更新估计值来逼近真实值。同样也是无模型，所以不需要状态转移矩阵或者是奖励函数。

**基本思想**：对于某个给定的策略 $\pi$,在线(online) 的、通过当前的估计值与接下来的估计值之间的差异来更新值函数，而不是等到最终结果出现后再进行更新。

最简单的算法是一步时序差分(one-step TD),即TD(0)。每往前走一步，就做一步自举，用得到的估计回报 (estimated return) $r_{t+1}+\gamma V(s_{t+1})$ 而不是$G_t$来更新上一时刻的值$V(s_t){:}$

$$
V\left(s_t\right)\leftarrow V\left(s_t\right)+\alpha\left(r_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_t\right)\right)
$$
 
估计回报$r_{t+1}+\gamma V(s_{t+1})$被称为时序差分目标(TD target), 时序差分目标是带衰减的未来奖励的总和。
时序差分目标由两部分组成：
 (1) 我们走了某一步后得到的实际奖励$r_{t+1}$;
 (2) 我们利用了自举的方法，通过之前的估计来估计$V(s_{t+1})$,并且加了折扣因子，即 $\gamma V(s_{t+1})$。
 
 时序差分目标是估计有两个原因：
 (1) 时序差分方法对期望值进行采样；
 (2) 时序差分方法使用当前估计的$V$ 而不是真实的$V_\mathrm{\pi}$。
 时序差分误差(TD error) $\delta=r_{t+1}+\gamma V(s_{t+1})-V(s_t)$。
 类比增量式蒙特卡洛方法，给定一个回合 $i$ ,我们可以更新$V(s_t)$ 来逼近真实的回报 $G_t$, 具体更新公式为
$$
V\left(s_t\right)\leftarrow V\left(s_t\right)+\alpha\left(G_{i,t}-V\left(s_t\right)\right)
$$
### 时序差分和增量MC的对比
在蒙特卡洛方法里面，$G_(i,t)$  是实际得到的值（可以看成目标），因为它已经把一条轨迹跑完了，可以算出每个状态实际的回报。然而时序差分不等轨迹结束，往前走一步，就可以更新价值函数。
![[Pasted image 20231117220840.png]]

### n步时序差分
TD(n) 是对 TD(0) 方法的扩展，它考虑未来多个时间步的奖励和状态值。
在 TD(0) 中，只考虑下一步的奖励和下一步状态的值来更新当前状态的值函数。而在 TD(n) 中，会考虑未来 n 步的奖励和状态值，然后用这些信息来更新当前状态的值函数。
$$\begin{aligned}
&n=1(\mathrm{TD})\quad G_t^{(1)}=r_{t+1}+\gamma V\left(s_{t+1}\right) \\
&n=2\quad G_t^{(2)}=r_{t+1}+\gamma r_{t+2}+\gamma^2V\left(s_{t+2}\right) \\
&\begin{aligned}&n=\infty(\mathrm{MC})\quad G_t^\infty=r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{T-t-1}r_T\end{aligned}
\end{aligned}$$

n趋近无穷大的时候，就变成了蒙特卡罗方法。
解释：
当 $n$ 趋近无穷大时，TD($n$) 方法的更新规则会逐渐变得类似于蒙特卡罗方法。为了理解这一点，让我们看一下 TD($n$) 的值函数更新规则：

$$V(s_t) \leftarrow V(s_t) + \alpha \left[ (r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \ldots + \gamma^n r_{t+n} + \gamma^{n+1}V(s_{t+n+1})) - V(s_t) \right]$$

当 $n$ 趋近无穷大时，更新规则中的 $\gamma^{n+1}V(s_{t+n+1})$ 项的影响会逐渐减小，因为 $\gamma^{n+1}$ 会趋近于零。这意味着在计算更新时，我们越来越多地考虑未来奖励的累积和，而 $\gamma^{n+1}V(s_{t+n+1})$ 这一项的贡献变得相对较小。

当 $n$ 趋近无穷大时，整个更新规则可以近似为：

$$V(s_t) \leftarrow V(s_t) + \alpha \left[ G_t - V(s_t) \right]$$

其中 $G_t$ 是从时间步 $t$ 开始的未来奖励的累积和，这正是蒙特卡罗方法中所考虑的。在蒙特卡罗方法中，我们通过完整的回合经验来估计状态值函数，而 $G_t$ 就是回合的回报。

因此，当 $n$ 趋近无穷大时，TD($n$) 方法的更新规则逐渐收敛到蒙特卡罗方法。这也说明了蒙特卡罗方法是一种极端情况下的 TD($n$) 方法，其中 $n$ 等于回合的长度。

## 比较自举和采样

自举是指更新时使用了估计。
采样是指更新时通过采样得到一个期望。 

动态规划是自举。
蒙特卡洛方法是纯采样。
时序差分目标由两部分组成，一部分是采样，一部分是自举。

## 比较DP，MC，TD
![[Pasted image 20231117224731.png]]

![[Pasted image 20231126155648.png]]


1. **更新时机：**
   - **时序差分（TD）：** 在每个时间步进行更新。TD方法通过每一步的即时奖励和下一步状态的值来更新当前状态的值函数。
   - **动态规划（DP）：** 在每个时间步进行更新。DP方法使用问题的数学模型，通过递归地更新值函数，最终收敛到最优值函数。
   - **蒙特卡洛（MC）：** 在整个回合结束后进行更新。MC方法通过整个回合的回报来更新值函数。

2. **样本使用：**
   - **时序差分（TD）：** 每一步都产生一个更新，依赖于当前步的即时奖励和下一步状态的值。
   - **动态规划（DP）：** 使用整个问题的模型，通过递归地利用当前状态和下一步状态的值进行更新。
   - **蒙特卡洛（MC）：** 使用整个回合的轨迹进行更新，依赖于整个回合的经验。

3. **计算效率：**
   - **时序差分（TD）：** 相对较高的计算效率，因为每一步都进行更新。
   - **动态规划（DP）：** 可能计算开销较大，特别是在状态空间较大的情况下，因为需要递归地考虑所有可能的状态。
   - **蒙特卡洛（MC）：** 中等计算效率，因为需要等待整个回合结束后才能进行更新。

4. **方差：**
   - **时序差分（TD）：** 通常有中等方差，介于动态规划和蒙特卡洛之间。
   - **动态规划（DP）：** 通常有较低的方差，因为使用问题的模型进行更新。
   - **蒙特卡洛（MC）：** 通常有较高的方差，因为依赖于整个回合的随机性。

5. **适用场景：**
   - **时序差分（TD）：** 适用于在线学习和实时系统，不需要等待整个回合结束。
   - **动态规划（DP）：** 适用于离线学习，可以计算所有可能状态的值函数。
   - **蒙特卡洛（MC）：** 适用于需要考虑整个回合的情况，如棋类游戏或需要考虑长期影响的任务。

# 免模型控制

什么是控制？[[1.基础定义+介绍#控制（Control）：]]
总结来说就是找到最优策略和它的最优价值函数。但我们这里的控制是免模型的，即不知道他的状态转移概率，也就无法用前面的价值迭代和策略迭代来找到最优策略。


## 广义策略迭代

可以查看前文的策略迭代。[[2.MDP+DP#迭代]]
广义策略迭代（generalized policy iteration，GPI）是一种**强化学习的框架**，它将强化学习算法(比如蒙特卡洛方法和时序差分方法也加入)组合在一起，以实现对策略和值函数的同时迭代优化。

### 基本策略迭代
GPI包括两个基本的组件：**策略评估和策略改进**。通过交替地进行这两个步骤来达到最优策略。
**策略迭代（Policy Iteration）：**
   - **方法：** 交替进行策略评估和策略改进，直到策略不再改变。策略迭代保证最终收敛到最优策略和最优值函数。
   - **步骤：**
      1. 初始策略 $\pi_0$.
      2. 迭代进行策略评估，计算 $V^{\pi_i}$.
      3. 迭代进行策略改进，得到新的策略 $\pi_{i+1}$.
      4. 重复步骤2和步骤3，直到策略不再改变。

但是在计算策略改进时，$\pi'(s) = \text{argmax}_a Q^\pi(s, a)$，$Q_{\pi_i}(s,a)=R(s,a)+\gamma\sum_{s^{\prime}\in S}P\left(s^{\prime}\mid s,a\right)V_{\pi_i}\left(s^{\prime}\right)$
因为不知道R和P，所以无法进行策略改进。

### 改进➡广义
**改进**：改变策略评估，使用蒙特卡洛的方法代替动态规划的方法估计 Q 函数。

$$\pi(s)=\arg\max_aQ(s,a)$$




![[Pasted image 20231118183041.png]]
算法通过蒙特卡洛方法产生很多轨迹，每条轨迹都可以算出它的价值。然后，我们可以通过平均的方法去估计 Q 函数。Q 函数可以看成一个Q表格，我们通过采样的方法把表格的每个单元的值都填上，然后使用策略改进来选取更好的策略。 如何用蒙特卡洛方法来填 Q 表格是这个算法的核心。

![[Pasted image 20231118183141.png]]


### 改进➡epsilon-greedy

ε-greedy（epsilon-greedy）是一种在强化学习中用于在探索（exploration）和利用（exploitation）之间取得平衡的策略。在强化学习中，代理（或智能体）在学习过程中需要在已知最优策略的基础上进行利用，同时也需要探索未知策略，以便发现更好的策略。

ε-greedy策略以概率ε随机选择一个动作，以概率1-ε选择当前估计为最优的动作。具体来说，对于一个给定的状态，ε-greedy策略的行为如下：

- 以概率ε选择随机动作（探索）。
- 以概率1-ε选择当前被估计为最优的动作（利用）。

这里的ε是一个在0到1之间的小数，表示探索的概率。通常，ε的值会被设置为一个较小的数，例如0.1或0.2，以确保大部分时间都在利用当前最优策略，但也会有一些时间用于探索新的动作，以便更全面地了解环境并找到潜在的更好策略。
![[Pasted image 20231118183449.png]]

## Q-learning

Q-Learning is an **off-policy(离线)** **value-based（基于价值）** method that uses a **TD approach（TD方法）** to train its action-value function.
- off-policy:离线
- value-based:基于价值的方法，也就是说通过训练价值或动作-价值函数来**间接**找到最优策略。所以主要目标是寻找最优价值函数。
- TD approach：updates its action-value function **at each step** instead of at the end of the episode.（每一步都更新，而不是等到回合结束）。

Q-Learning 主要是 **train our Q-function**。

>Q是什么？
> Q comes from “the Quality” (the **value**) of that action at that state.状态-动作的价值

value和reward的区别：
- value：状态或状态-动作对的值是我们的代理在从该状态（或状态-动作对）开始然后根据其策略采取行动时获得的预期累积奖励。
- reward：奖励是我在某个状态执行操作后从环境获得的反馈。

$$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma\max_aQ(s_{t+1},a)-Q(s_t,a_t)]$$

### Q表格
Q表格是用于存储每个状态-动作对应的Q值。
Q表格是Q-Learning算法中一个重要的组成部分，用于在学习过程中记录和更新这些Q值。

Q表格的行对应于不同的状态，而列对应于可选的动作。
例如，对于状态集合{S1, S2, S3}和动作集合{A1, A2, A3}，Q表格可能如下所示：

$$
\begin{array}{cccc}
       & \text{A1} & \text{A2} & \text{A3} \\
\text{S1} & Q(S1, A1) & Q(S1, A2) & Q(S1, A3) \\
\text{S2} & Q(S2, A1) & Q(S2, A2) & Q(S2, A3) \\
\text{S3} & Q(S3, A1) & Q(S3, A2) & Q(S3, A3) \\
\end{array}

$$
the Q-function uses a Q-table that has the value of each state-action pair. Given a state and action, our Q-function will search inside its Q-table to output the value.（Q 函数使用具有每个状态-动作对的值的 Q 表。给定状态和动作，我们的 Q 函数将在其 Q 表内搜索以输出值。）
![[Pasted image 20231126173558.png]]



### Q-learning的训练
- Trains a Q-function (an action-value function), which internally is a Q-table that contains all the state-action pair values.（训练 Q 函数（动作值函数），其内部是包含所有状态动作对值的 Q 表。）
- Given a state and action, our Q-function will search its Q-table for the corresponding value.（给定一个状态和动作，我们的 Q 函数将在其 Q 表中搜索相应的值。）
- When the training is done, we have an optimal Q-function, which means we have optimal Q-table.（训练完成后，我们就有了最优的 Q 函数，这意味着我们有最优的 Q 表。）
- And if we have an optimal Q-function, we have an optimal policy since we know the best action to take at each state.（如果我们有一个最优的 Q 函数，那么我们就有一个最优的策略，因为我们知道在每个状态下采取的最佳行动。）

![[Pasted image 20231118180330.png]]

总结Q-learning算法步骤：
step1：初始化Q表，可以把每个地方初始化为0。
step2：使用 epsilon-greedy 策略选择一个操作。
step3：执行动作At，获得奖励$R_{t+1}$和下一个状态$S_{t+1}$
step4：更新 $Q(S_t, A_t)$。
	attention：在 TD 学习中，我们在交互的一步后更新我们的策略或价值函数。
	在TD的目标中，我们用贪婪策略，要选择在下一个状态下具有最大Q值的动作。$$
V\left(s_t\right)\leftarrow V\left(s_t\right)+\alpha\left(r_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_t\right)\right)
$$
	所以我们要找到在下一个状态下最大化当前 Q 函数的动作。所以更新公式如下：
	![[Pasted image 20231126194601.png]]

重新思考一下：
- 如果要更新$Q(S_t,A_t)$ ,那么我们需要$(S_t,A_t,S_{t+1},A_{t+1})$。
- 为了更新给定状态-动作对的 Q 值，我们使用 TD 目标。
	如何指定TD目标？
	- 我们采取$A_t$ 之后将获得 $R_{t+1}$
	- 为了获得下一个状态最佳的状态-动作对，我们采用贪心去选择下一个动作。（不是使用epsilon 贪婪策略）
- 当Q值完成之后，我们从一个新的状态开始，再次使用 epsilon-greedy 策略选择我们的动作。



## Sarsa


$$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]$$

![[Pasted image 20231118180350.png]]