# 背景
之前我们只研究了基于价值的强化学习，其中我们估计价值函数作为寻找最优策略的中间步骤。[[1.基础定义+介绍#两大方法——基于策略和基于价值]]
需要找到一个最优函数，使得拥有最优策略。
$$\pi^*(s)=\arg\max_aQ^*(s,a)$$
在基于价值的方法中，策略（$\pi$）仅因动作价值估计而存在，因为策略只是一个函数（例如，贪婪策略），它将选择给定状态下具有最高价值的动作。

如果我们想要学习基于策略的强化学习，那么我们**希望直接优化策略**，而不需要学习价值函数的中间步骤。

# 策略梯度
强化学习的主要目标：找到能够最大化预期累积奖励的最优策略 $\pi^*$ 。

在基于价值的强化学习中：
1. 目标：逼近真实的行动价值函数，最小化预测值和目标值之间的损失。
2. 策略：是先找到最有价值，再从最有价值中提取最优策略。（比如在Q-learning中我们采用了($\epsilon$-)greedy policy策略）

基于策略的强化学习中：直接学习近似$\pi^*$ ,不管价值函数。
	方法：我们可以**参数化策略**（parameterize the policy）。例如，使用神经网络 $\pi_\theta$ 拟合该策略，将输出动作的概率分布（随机策略）。

1. 目标：使用梯度上升最大化参数化策略的性能。

举例：
![[Pasted image 20231127195227.png]]


## 区别策略和策略梯度
策略梯度方法是基于策略的方法的子类。

区别在于如何优化$\theta$
- 在基于策略(policy-based)的方法中，我们直接搜索最优策略。可以通过爬山、模拟退火或进化策略等技术最大化目标函数的局部近似来间接优化参数。
- 在基于策略梯度(policy-gradient)的方法中，直接通过对目标函数 $J(\theta)$ 的性能进行梯度上升来优化参数 $\theta$ .

本章主要讨论的是策略梯度，也就是策略方法的子类。
## 优缺点
### 优点
- 可以直接估计策略，而无需存储额外的数据（动作价值Q）
- 策略梯度可以学习随机策略，而价值函数则不能。
	- **策略梯度**：在相同状态下选择不同动作的概率都有，即策略可以是随机的。梯度更新可以使得**更优越的动作的概率增加**，从而提高整体性能。
	- **值函数**：通常使用**贪婪策略**，即在**给定状态下选择最大化值函数的动作**。这意味着对于相同的状态，它倾向于选择一个确定性的动作，而不是一个以概率分布形式表示的随机策略。
由此带来的好处：
1. 我们不需要手动实现探索/利用权衡。由于我们输出动作的概率分布，代理探索状态空间而不总是采取相同的轨迹。（比如Q-learning就需要手动去实现$\epsilon$-贪婪策略）
2. 摆脱了感知混叠的问题。感知混叠是指两个状态看起来（或）相同但需要不同的动作。
3. 策略梯度方法在高维行动空间和连续行动空间中更有效。
4. 策略梯度方法具有更好的收敛性。
	对于基于价值的强化学习来说，由于我们要找 Q 估计的最大值。因此，如果估计动作值的任意小变化导致具有最大值的不同动作，则该动作概率可能会发生显着变化。
	但是对于策略梯度来说，只是变化概率，会有更好的连贯性。


### 缺点

- 策略梯度方法会收敛到局部最大值而不是全局最优值。
- 训练可能需要更长的时间（效率低下）。
- 策略梯度可能具有高方差。


## 深入了解策略梯度

策略梯度方法旨在找到最大化预期回报的参数 $\theta$。

