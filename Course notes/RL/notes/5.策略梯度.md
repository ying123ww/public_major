# 背景
之前我们只研究了基于价值的强化学习，其中我们估计价值函数作为寻找最优策略的中间步骤。[[1.基础定义+介绍#两大方法——基于策略和基于价值]]
需要找到一个最优函数，使得拥有最优策略。
$$\pi^*(s)=\arg\max_aQ^*(s,a)$$
在基于价值的方法中，策略（$\pi$）仅因动作价值估计而存在，因为策略只是一个函数（例如，贪婪策略），它将选择给定状态下具有最高价值的动作。

如果我们想要学习基于策略的强化学习，那么我们**希望直接优化策略**，而不需要学习价值函数的中间步骤。

# 策略梯度
强化学习的主要目标：找到能够最大化预期累积奖励的最优策略 $\pi^*$ 。

在基于价值的强化学习中：
1. 目标：逼近真实的行动价值函数，最小化预测值和目标值之间的损失。
2. 策略：是先找到最有价值，再从最有价值中提取最优策略。（比如在Q-learning中我们采用了($\epsilon$-)greedy policy策略）

基于策略的强化学习中：直接学习近似$\pi^*$ ,不管价值函数。
	方法：我们可以**参数化策略**（parameterize the policy）。例如，使用神经网络 $\pi_\theta$ 拟合该策略，将输出动作的概率分布（随机策略）。

1. 目标：使用梯度上升最大化参数化策略的性能。

举例：
![[Pasted image 20231127195227.png]]


## 区别策略和策略梯度
策略梯度方法是基于策略的方法的子类。

区别在于如何优化$\theta$
- 在基于策略(policy-based)的方法中，我们直接搜索最优策略。可以通过爬山、模拟退火或进化策略等技术最大化目标函数的局部近似来间接优化参数。
- 在基于策略梯度(policy-gradient)的方法中，直接通过对目标函数 $J(\theta)$ 的性能进行梯度上升来优化参数 $\theta$ .

本章主要讨论的是策略梯度，也就是策略方法的子类。
## 优缺点
### 优点
- 可以直接估计策略，而无需存储额外的数据（动作价值Q）
- 策略梯度可以学习随机策略，而价值函数则不能。
	- **策略梯度**：在相同状态下选择不同动作的概率都有，即策略可以是随机的。梯度更新可以使得**更优越的动作的概率增加**，从而提高整体性能。
	- **值函数**：通常使用**贪婪策略**，即在**给定状态下选择最大化值函数的动作**。这意味着对于相同的状态，它倾向于选择一个确定性的动作，而不是一个以概率分布形式表示的随机策略。
由此带来的好处：
1. 我们不需要手动实现探索/利用权衡。由于我们输出动作的概率分布，代理探索状态空间而不总是采取相同的轨迹。（比如Q-learning就需要手动去实现$\epsilon$-贪婪策略）
2. 摆脱了感知混叠的问题。感知混叠是指两个状态看起来（或）相同但需要不同的动作。
3. 策略梯度方法在高维行动空间和连续行动空间中更有效。
4. 策略梯度方法具有更好的收敛性。
	对于基于价值的强化学习来说，由于我们要找 Q 估计的最大值。因此，如果估计动作值的任意小变化导致具有最大值的不同动作，则该动作概率可能会发生显着变化。
	但是对于策略梯度来说，只是变化概率，会有更好的连贯性。


### 缺点

- 策略梯度方法会收敛到局部最大值而不是全局最优值。
- 训练可能需要更长的时间（效率低下）。
- 策略梯度可能具有高方差。


## 深入了解策略梯度

策略梯度方法旨在找到最大化预期回报的参数 $\theta$。

例子：
![[Pasted image 20231127212118.png]]

输入：有一个状态
输出：该状态下动作的概率分布
策略梯度目标：通过调整策略（即优化权重）来控制动作的概率分布，以便将来更频繁地采样好的动作（最大化回报）

问题：如何使用预期回报来优化权重？
agent interact during an episode，如果win，则所采取的每项行动都是好的，并且必须在未来进行更多的尝试。如果不好，就减少尝试。
所以对于每个状态-动作对，增加$P(a \mid s)$,即在该状态下采取该动作的概率。

简化的策略梯度算法：
![[Pasted image 20231127212654.png]]

我们有随机策略 $\pi$ ，它有一个参数 $\theta$ 。给定一个状态$s$，这个 $\pi_\theta$ 输出动作 $A$的概率分布。
$$\begin{aligned}\pi_\theta(s)=\mathbb{P}[A|s;\theta]\end{aligned}$$
如何评判政策好不好？定义分数/目标 函数$J(\theta)$。
## 目标价值函数

目标函数为我们提供给定轨迹（不考虑奖励的状态动作序列（与情节相反））的智能体的性能，并输出预期的累积奖励。
目标价值函数：
$$J(\theta)=\sum_{\tau}R(\tau)P_\theta(\tau)=E_{\tau\sim P_{\theta}(\tau)}[R(\tau)]$$

$\tau$ ：轨迹，是动作和状态的序列。$\tau=\{s_1,a_1,s_2,a_2,\cdots,s_t,a_t\}$
$R(\tau)$ : 累计回报。
	![[Pasted image 20231129175809.png]]
$P_\theta(\tau)$：每个可能轨迹 $\tau$ 的概率。
	![[Pasted image 20231129180239.png]]

**目标**：目标是通过找到输出最佳动作概率分布的 $\theta$ 来最大化预期累积奖励。
	$$maxJ(\theta)=\mathbb{E}_{\tau\sim\pi_\theta}[R(\tau)]$$
	转化为一个优化问题。
## 解最优化问题
### 梯度上升
一个优化问题：
	**目的**：想要找到$\theta$, 使得目标函数$J(\theta)$最大化。
	**方法**：梯度上升。（是梯度下降的反函数，提供了最陡的上升方向）

$$\theta\leftarrow\theta+\alpha*\nabla_\theta J(\theta)$$

根据公式，出现了关于$J(\theta)$ 的导数。于是出现以下两个问题：
	1. 我们无法计算目标函数的真实梯度，因为它需要计算每个可能轨迹的概率，这在计算上是非常昂贵的。所以我们想用**基于样本的估计来计算梯度估计**（收集一些轨迹）。
	2. 为了区分这个目标函数，我们需要区分状态分布，称为马尔可夫决策过程动力学。这与环境有关。给定当前状态和所采取的操作，它为我们提供了环境进入下一个状态的概率。问题是我们无法区分它，因为我们可能不知道它。

解决：使用**策略梯度定理**，它将帮助我们将目标函数重新表述为**不涉及状态分布微分的可微函数**。

### 策略梯度
调整后的策略梯度：
$$\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(a_t|s_t)R(\tau)]$$

推导如下：
***
两个主要公式：
1.目标价值函数梯度：$$\nabla_\theta J(\theta)=\nabla_\theta\sum_\tau P(\tau;\theta)R(\tau)$$
2. 轨迹概率：$$P(\tau;\theta)=[\prod_{t=0}P(s_{t+1}|s_t,a_t)\pi_\theta(a_t|s_t)]$$

应为和的梯度是梯度的和，所以将梯度移入，再乘以1，即$\frac{P(\tau;\theta)}{P(\tau;\theta)}$ 。
$$
\begin{aligned}
\nabla_\theta J(\theta)&=\nabla_\theta\sum_\tau P(\tau;\theta)R(\tau)\\
&=\sum_\tau\nabla_\theta P(\tau;\theta)R(\tau)\\
&=\sum_{\tau}\frac{P(\tau;\theta)}{P(\tau;\theta)}\nabla_\theta P(\tau;\theta)R(\tau)\\
&=\sum_\tau P(\tau;\theta)\frac{\nabla_\theta P(\tau;\theta)}{P(\tau;\theta)}R(\tau)\\
\end{aligned}

$$
又因为log的性质：$\begin{aligned}\nabla_xlogf(x)=\frac{\nabla_xf(x)}{f(x)}\end{aligned}$，所以$\frac{\nabla_\theta P(\tau;\theta)}{P(\tau;\theta)}=\nabla_\theta logP(\tau|\theta)$:
所以：
$$\nabla_\theta J(\theta)=\sum_\tau P(\tau;\theta)\nabla_\theta logP(\tau;\theta)R(\tau)$$
对于用有限样本来预估，即得到这个公式：
$$
\nabla_\theta J(\theta)=\frac1m\sum_{i=1}^m\nabla_\theta logP(\tau^{(i)};\theta)R(\tau^{(i)})
$$


但是对于$\nabla_\theta logP(\tau|\theta)$ 还是可以**很好的化简**：
首先利用公式2得到：
$$\nabla_\theta logP(\tau^{(i)};\theta)=\nabla_\theta log[\mu(s_0)\prod_{t=0}^HP(s_{t+1}^{(i)}|s_t^{(i)},a_t^{(i)})\pi_\theta(a_t^{(i)}|s_t^{(i)})]$$

由于log（乘积）=求和log（），所以：

$$
\begin{aligned}
\nabla_\theta logP(\tau^{(i)};\theta)&=\nabla_\theta\left[log\mu(s_0)+\sum_{t=0}^HlogP(s_{t+1}^{(i)}|s_t^{(i)}a_t^{(i)})+\sum_{t=0}^Hlog\pi_\theta(a_t^{(i)}|s_t^{(i)})\right]
\end{aligned}
$$

由于导数【求和】=【求和】导数，于是可以提入，但是状态转移概率和第一项都和$\theta$ 无关，所以求导直接为0：
$$
\begin{aligned}
\nabla_\theta logP(\tau^{(i)};\theta)&=
\nabla_\theta log\mu(s_0)+\nabla_\theta\sum_{t=0}^HlogP(s_{t+1}^{(i)}|s_t^{(i)}a_t^{(i)})+\nabla_\theta\sum_{t=0}^Hlog\pi_\theta(a_t^{(i)}|s_t^{(i)})\\
&=\sum_{t=0}^H\nabla_\theta log\pi_\theta(a_t^{(i)}|s_t^{(i)})\\
\end{aligned}
$$

所以将上述带入得到：
$$\nabla_\theta J(\theta)=\frac1m\sum_{i=1}^m\sum_{t=0}^H\nabla_\theta\log\pi_\theta(a_t^{(i)}|s_t^{(i)})R(\tau^{(i)})$$

## 最优化算法——蒙特卡洛强化
Monte-Carlo 策略梯度，又称REINFORCE算法，是一种策略梯度算法，它使用整个episode的估计回报来更新策略参数 $\theta$ 。
![[Pasted image 20231129182011.png]]




重新看一下MC和TD的对比：
![[Pasted image 20231129181907.png]]

