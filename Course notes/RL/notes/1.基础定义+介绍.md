
# RL定义+基本要素
强化学习（Reinforcement Learning，简称RL）是一种机器学习方法。
目标是通过代理与环境的交互学习，以实现某种目标或最大化累积的奖励信号。
如图：在强化学习中，代理根据环境的状态采取行动，并接收一个奖励信号作为反馈，通过学习从先前的经验中提取知识，逐步改进其策略，以达到最优的决策策略。
![[Pasted image 20231114000035.png]]

强化学习的基本要素包括：
1. **代理（Agent）：** 学习和决策的主体，它负责感知环境、选择行动，并通过学习来提高其性能。
2. **环境（Environment）：** 代理与之交互的外部系统或情境，其状态可能受到代理的行动影响。
3. **状态（State）：** 描述环境的特定瞬时情况，影响代理选择下一步行动的依据。
4. **行动（Action）：** 代理基于当前状态所采取的决策或操作。
5. **奖励（Reward）：** 表示代理在特定状态下采取特定行动的好坏程度的信号，用于指导代理优化其策略。

一个序列决策过程：智能体与环境进行交互，它在每个时间步(t)都会观察到一个状态（state）$s_t$，然后根据某种策略（policy）选择一个动作（action）$a_t$。执行动作后，环境会根据动作和当前状态转移到新的状态，并提供一个奖励（reward）信号 $r_{t+1}$。

结合基本要素重述强化学习主要目标：找到一种最优策略，即在不同状态下采取的最佳行动，以使累积奖励最大化。这一过程通常通过价值函数来衡量，价值函数评估代理在某个状态下的长期回报。

- **强化学习方法**：基于价值的方法、基于策略的方法以及这两者的结合。
- **DRL**：深度强化学习结合了深度学习和强化学习，通过深度神经网络来学习复杂的策略（实现端到端。
- **状态和观测**：状态是环境的内部表示，而观测是智能体从环境中获取的信息。状态是环境的全貌，观测有时候是全部，有时候是部分。如果环境是完全可观测的，则可以建模为马尔可夫决策过程（MDP）


## 两大方法——基于策略和基于价值

**例子：走迷宫**

假设有一个迷宫，智能体需要学习如何从起始点到达目标点。在每个格子里，智能体可以选择四个动作：向上、向下、向左或向右。每个格子都有一个奖励，目标是找到一种策略，使得智能体在整个迷宫中的总奖励最大。

1. **基于策略的方法：**

   - **策略表示：** 智能体学习一个策略，即在每个格子处选择一个动作的概率分布。例如，对于某个格子，策略可以是\[0.2, 0.4, 0.1, 0.3\]，表示分别选择四个动作的概率。
   
   - **目标：** 优化策略参数，使得整个路径上的动作选择能够最大化累积奖励。

   - **优点：** 可以直接处理连续动作空间，例如，在每个格子处选择动作的概率。

2. **基于价值的方法：**

   - **价值函数表示：** 智能体学习一个价值函数，即在每个格子处采取某个动作后预期获得的累积奖励。例如，对于某个格子和动作，价值函数可以是一个实数。

   - **目标：** 找到最优的值函数，即对于每个状态或状态-动作对，价值函数能够指导智能体做出最优决策。

   - **优点：** 在处理部分可观测问题时可能更为有效。

基于策略和基于价值的强化学习方法是两种不同的方法，它们分别关注于如何表示和学习智能体在环境中做出决策的不同方面。

### 基于策略的强化学习（Policy-Based Reinforcement Learning）：

1. **策略表示：** 基于策略的方法直接学习策略，即给定状态，智能体应该采取的动作的概率分布。策略通常用符号或参数化的形式来表示。

2. **目标：** 目标是找到一个最优的策略，使得在整个任务中累积的期望奖励最大化。优化的焦点是策略的参数，以使得选择的动作在长期内最有可能导致高累积奖励。

3. **优点：** 策略方法适用于高度随机或连续动作空间，而且它们能够直接处理探索-利用的平衡问题。

4. **例子：** 攭者梯度法（Policy Gradient Methods）是基于策略的一类方法。

### 基于价值的强化学习（Value-Based Reinforcement Learning）：

1. **价值函数表示：** 基于价值的方法关注于学习状态或状态-动作对的价值函数，该函数表示在当前状态或状态-动作对上采取行动能够获得的累积奖励。

2. **目标：** 目标是找到最优的值函数，即找到每个状态或状态-动作对的最大累积奖励。策略可以通过使用值函数来推导，例如选择使值函数最大化的动作。

3. **优点：** 价值方法在处理部分可观测问题（部分可观测马尔可夫决策过程，POMDP）时通常更为有效。

4. **例子：** Q-learning 和深度 Q 网络（DQN）是基于价值的方法的例子。

**In value-based methods, rather than learning the policy, we define the policy by hand and we learn a value function. If we have an optimal value function, we will have an optimal policy.
在基于价值的方法中，我们不是学习策略，而是手动定义策略并学习价值函数。如果我们有一个最优价值函数，我们就会有一个最优策略。**
### 区别总结：

With Policy-Based methods, we train the policy directly to learn which action to take given a state.
With value-based methods, we train a value function to learn which state is more valuable and use this value function to take the action that leads to it.

- 基于策略的方法直接学习决策策略，而基于价值的方法学习状态或状态-动作对的价值函数。
- 策略方法更适用于处理连续或高度随机的动作空间，而价值方法在处理部分可观测问题时可能更有效。
- 策略方法直接处理探索-利用的平衡问题，而价值方法通常需要额外的探索策略。

![[Pasted image 20231119202346.png]]
# 两大模型——有模型和免模型
免模型强化学习（Model-Free Reinforcement Learning）和有模型强化学习（Model-Based Reinforcement Learning）的主要区别在于它们处理环境模型的方式以及在学习中是否使用模型。

### 1. **免模型强化学习（Model-Free RL）：**
![[Pasted image 20231117144500.png]]


   - **特点：** 免模型强化学习不依赖于显式的环境模型。智能体（agent）从与环境的交互中学习，通过尝试不同的动作并观察环境的反馈来更新策略或值函数。
   - **方法：** 代表性的方法包括Q-learning、SARSA、深度强化学习（如深度Q网络 - DQN）等。这些方法直接从经验中学习，而**不需要显式地建模环境的动态特性**。

### 2. **有模型强化学习（Model-Based RL）：**
![[Pasted image 20231117144443.png]]
   - **特点：** 有模型强化学习涉及构建对环境的显式模型，并使用该模型来规划未来的行为。这个模型通常包括对状态转移和奖励的估计，即环境的动态。（状态转移概率已知）
   - **方法：** 典型的方法包括动态规划（DP），即基于模型的价值迭代（Model-Based Value Iteration）、基于模型的策略迭代（Model-Based Policy Iteration）等。这些方法使用模型来模拟环境，然后使用规划算法（如值迭代或策略迭代）来找到最优策略。


### 比较总结：
- 在免模型强化学习中，智能体直接从经验中学习，无需事先知道环境的动态。
- 在有模型强化学习中，智能体试图建立一个环境的模型，并使用该模型进行规划，以获得更有效的策略。
- **模型：** 有模型方法关注于学习环境的模型，而免模型方法直接学习策略或价值函数。

![[Pasted image 20231119202241.png]]


# 两个交互——学习与规划

### 学习（Learning）：

学习是指智能体通过与环境的交互，从经验中提取信息、改进策略或价值函数，以提高在未来任务中的性能。

1. **模型学习：** 智能体学习环境的模型，即对环境动态的内部表示。这包括**对状态转移概率和奖励函数的估计**。**有模型学习**通常与规划过程结合使用，使智能体能够通过模拟环境来评估可能的行动。

2. **策略学习或价值函数学习：** 智能体直接学习执行动作的策略或学习状态或状态-动作对的价值函数。这类方法通常涉及使用**免模型学习**算法，例如 **Q-learning、深度 Q 网络（DQN）或策略梯度**等。

### 规划（Planning）：

规划是指智能体使用学到的知识，通过在环境模型或实际环境中模拟未来的状态、动作和奖励，制定最优的行动策略。比如策略迭代或者价值迭代。

1. **模型规划：** 使用学习到的环境模型，智能体通过规划算法来模拟不同动作的影响，然后选择使得长期累积奖励最大化的动作。有模型强化学习方法通常涉及到模型规划。

2. **无模型规划：** 在没有显式环境模型的情况下，智能体通过使用学到的策略或价值函数，使用规划算法来选择最佳的行动。这通常是免模型强化学习方法所采用的方式。

### 学习与规划的关系：

- **相互作用：** 学习和规划通常是相互作用的过程。学到的知识可以用于规划，而规划的结果又可以用于更新学习的模型、策略或价值函数。

- **探索与利用：** 学习阶段通常涉及探索环境以获取更多信息，而规划阶段则侧重于利用学到的知识来做出决策。


# 两个核心——预测和控制
MDP（Markov Decision Process，马尔可夫决策过程）是一种数学框架，用于描述包含状态、动作、奖励和转移概率等元素的决策问题。在MDP中，有两个主要的概念：预测（prediction）和控制（control）。

## 预测（Prediction）：
预测指的是在给定某个策略（policy）下，对MDP的**未来状态和奖励进行估计或预测**。具体来说，预测问题是关于**评估系统在当前策略下的性能有多好的问题**。解决预测问题的方法通常包括**值函数**（value function）的计算，**比如估计状态值函数或者是动作值函数**，其中值函数表示每个状态或状态-动作对的长期累积奖励。

## 控制（Control）：
控制是指在MDP中找到一个最优的策略，使得系统在长期内获得**最大的累积奖励**。控制问题涉及到在每个状态下选择最优的动作，从而达到最优的长期累积奖励。解决控制问题的方法包括价值迭代（value iteration）、策略迭代（policy iteration）等。

## 比较总结：
总体来说，预测解决了“给定一个策略，系统将来会怎样”的问题，而控制解决了“找到一个最优的策略，使得系统在未来获得最大累积奖励”的问题。在强化学习中，MDP提供了一个理论基础，帮助智能体学习如何在一个环境中做出最优的决策。


# 两个策略——离线策略与在线策略
Off-policy vs On-policy
- Off-policy: using **a different policy** for acting (inference) and updating (training).（使用不同的策略来执行（推理）和更新（训练））
	例如，对于 Q-Learning，epsilon 贪婪策略（执行策略）与用于选择最佳下一状态操作值以更新 Q 值（更新策略）的贪婪策略不同。

- On-policy: using the **same policy** for acting and updating.（使用相同的策略进行操作和更新。）
	For instance, with Sarsa, another value-based algorithm, the epsilon-greedy policy selects the next state-action pair, not a greedy policy.（sarsa：epsilon 贪婪策略选择下一个状态-动作对）![[Pasted image 20231126195832.png]]

# 随机策略和确定性策略
在强化学习中，策略定义了智能体（agent）如何在给定状态下选择动作。策略是智能体与环境交互的核心，它指导智能体在特定情境下如何行动。策略可以分为两类：随机策略和确定性策略。

1. **随机策略（Stochastic Policy）**：
   - 定义：在随机策略下，给定一个状态，智能体会根据一个概率分布来选择其动作。这意味着即使在同一状态下，智能体可能会选择不同的动作。
   - 特点：随机策略允许智能体进行探索（exploration），即尝试不同的动作以更好地了解环境。这在复杂或不确定的环境中特别重要。
   - 示例：在一个简单的迷宫游戏中，智能体可能会以一定的概率选择向左、向右、向上或向下移动，即使在同一位置。

2. **确定性策略（Deterministic Policy）**：
   - 定义：确定性策略是指在给定状态下，智能体总是选择相同的动作。换言之，策略为每个状态直接指定了一个动作。
   - 特点：确定性策略通常在已经充分探索环境并需要执行最优策略时使用。这种策略更为直接和简单，但可能缺乏探索，不一定适用于所有情况。
   - 示例：在自动驾驶车辆中，给定特定的交通状况和车辆状态，智能体可能总是做出相同的驾驶决策（如减速、加速或转向）。

总结：
- **随机策略**更适合于探索未知环境，因为它允许智能体尝试不同的动作并从中学习。
- **确定性策略**则更适用于已经了解的环境，智能体可以直接选择最优动作。

在实际应用中，强化学习算法可能会在探索（使用随机策略）和利用（使用确定性策略）之间进行权衡。

# RL算法
总体框架图：
![[Pasted image 20231119202141.png]]

# 所学的算法总结
确实，DQN、Actor-Critic、TRPO、PPO和DDPG这些强化学习算法之间存在着一定的关系和区别，同时它们确实采用了目标网络、训练网络、经验回放和Actor-Critic等不同的方法和技术。让我帮你梳理一下：

### DQN（Deep Q-Network）

1. **核心思想**：DQN是将深度学习应用于Q学习的一个突破。它使用深度神经网络来近似Q函数（动作价值函数）。
2. **关键技术**：
   - **经验回放**：通过存储智能体的经验，并从中随机抽样来训练网络，减少样本间的相关性。
   - **目标网络**：使用两个网络（一个目标网络和一个训练网络）来稳定训练过程。

### Actor-Critic

1. **核心思想**：Actor-Critic结合了值函数方法和策略梯度方法。它有两部分：演员（Actor）用于选择动作，评论家（Critic）用于评估动作。
2. **关键技术**：使用两个不同的网络（或网络部分），一个作为Actor，另一个作为Critic。

### TRPO（Trust Region Policy Optimization）

1. **核心思想**：TRPO是一种策略梯度方法，它通过限制策略更新的步长来确保每次更新是在信任域（trust region）内，从而避免过大的性能波动。
2. **关键技术**：优化策略的同时保持新旧策略之间的差异在一定范围内。

### PPO（Proximal Policy Optimization）

1. **核心思想**：PPO是对TRPO的简化和改进。它旨在简化实现，同时保持类似的性能。
2. **关键技术**：使用截断的梯度上升算法或添加约束项来限制策略更新。

### DDPG（Deep Deterministic Policy Gradient）

1. **核心思想**：DDPG是一种结合了Actor-Critic和DPG（Deterministic Policy Gradient）的算法，适用于连续动作空间。
2. **关键技术**：
   - **目标网络和训练网络**：与DQN类似，使用两套网络结构。
   - **经验回放**：和DQN类似，使用经验回放来提升训练效率和稳定性。
   - **Actor-Critic框架**：演员负责选择动作，评论家评估动作。


### SAC（Soft Actor-Critic）

1. **核心思想**：SAC是一种基于Actor-Critic框架的强化学习算法，它结合了深度学习和最大熵强化学习的理念。最大熵强化学习的目标不仅是最大化累积奖励，还要在此过程中增加策略的熵（即增加随机性），从而鼓励探索。

2. **关键技术**：
   - **最大熵强化学习**：SAC的目标是最大化奖励的同时最大化策略的熵，这有助于提高探索效率和学习更健壮的策略。
   - **Actor-Critic框架**：SAC采用双重结构，包括一个策略网络（Actor）和两个价值网络（Critic）。这有助于更准确地估计价值函数。
   - **连续动作空间**：SAC特别适合处理连续动作空间的问题，其性能在许多任务中表现出色。

3. **与其他算法的比较**：
   - 与**DDPG**和**TRPO**相比，SAC在探索方面更加高效，因为它不仅仅依赖于奖励，还考虑了策略的熵。
   - 相对于**PPO**和**TRPO**，SAC提供了更好的样本效率和学习稳定性，特别是在处理复杂的连续动作空间问题时。

### 总结

- **DQN**专注于值函数的近似，适用于离散动作空间。
- **Actor-Critic**提供了一种结合值函数和策略梯度方法的框架。
- **TRPO**和**PPO**是策略梯度方法的进阶版本，关注于策略的稳定更新。
- **DDPG**是Actor-Critic方法的一个变种，适用于**连续动作空间**，并借鉴了DQN的技术（如目标网络和经验回放）。

每种算法都有其适用的场景和特点，选择哪一种算法取决于具体问题的性质，如状态和动作的空间是否连续、问题的复杂性等。

# 基于策略梯度的发展线路
从基于策略梯度算法的发展角度来看，REINFORCE、Actor-Critic、TRPO、PPO、DDPG、SAC等算法展示了这一领域的不断演进和优化。这些算法在核心理念上都旨在优化策略（即智能体的行为准则），但它们在实现细节和应对强化学习中常见问题的方法上各有特点。

### 1. REINFORCE

- **基本概念**：REINFORCE是最基本的策略梯度算法。它直接根据策略产生的回报来更新策略参数，即使得奖励更高的动作被更频繁地选中。
- **特点**：简单直接，但效率低下，因为它高度依赖于回报的方差，且不使用价值函数作为辅助。

### 2. Actor-Critic

- **发展**：为了解决REINFORCE效率低下的问题，Actor-Critic算法被提出。它引入了价值函数作为Critic，以降低方差并稳定训练。
- **特点**：Actor负责策略（即动作选择），Critic评估这个动作的好坏（价值函数）。相比REINFORCE，它学习更快、更稳定。

### 3. TRPO（Trust Region Policy Optimization）

- **发展**：针对Actor-Critic算法可能在更新策略时产生过大变动的问题，TRPO被提出。它通过限制策略更新的步长来确保训练的稳定性。
- **特点**：使用了信任域来限制策略更新的幅度，保证每次更新不会偏离太远。

### 4. PPO（Proximal Policy Optimization）

- **发展**：PPO是对TRPO的进一步简化和改进。它旨在保持TRPO的优点，同时简化实现和计算。
- **特点**：通过裁剪策略梯度或添加目标函数的约束，避免了TRPO中复杂的优化问题。

### 5. DDPG（Deep Deterministic Policy Gradient）

- **发展**：DDPG结合了确定性策略（来自DPG）和Actor-Critic框架的优点，特别适用于连续动作空间。
- **特点**：使用深度学习来近似Actor和Critic，引入了目标网络和经验回放以增强稳定性和效率。

### 6. SAC（Soft Actor-Critic）

- **发展**：SAC是基于最大熵强化学习原理的算法，它在Actor-Critic基础上增加了熵作为额外的奖励，鼓励探索。
- **特点**：强调在奖励最大化的同时增加策略的熵（探索性），适合于复杂和不确定的环境。

### 总结

这些算法共同的发展线是如何更有效地优化策略和降低学习过程中的方差。从REINFORCE到Actor-Critic，再到TRPO、PPO、DDPG、SAC，每一步都在尝试解决前一代算法的局限性，如提高样本效率、减少方差、增强学习稳定性和提升探索能力。这些算法的选择和应用依赖于具体问题的需求，例如状态和动作空间的连续性、需要的计算资源、以及对效率和稳定性的需求。
