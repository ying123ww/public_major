
# RL定义+基本要素
强化学习（Reinforcement Learning，简称RL）是一种机器学习方法。
目标是通过代理与环境的交互学习，以实现某种目标或最大化累积的奖励信号。
如图：在强化学习中，代理根据环境的状态采取行动，并接收一个奖励信号作为反馈，通过学习从先前的经验中提取知识，逐步改进其策略，以达到最优的决策策略。
![[Pasted image 20231114000035.png]]

强化学习的基本要素包括：
1. **代理（Agent）：** 学习和决策的主体，它负责感知环境、选择行动，并通过学习来提高其性能。
2. **环境（Environment）：** 代理与之交互的外部系统或情境，其状态可能受到代理的行动影响。
3. **状态（State）：** 描述环境的特定瞬时情况，影响代理选择下一步行动的依据。
4. **行动（Action）：** 代理基于当前状态所采取的决策或操作。
5. **奖励（Reward）：** 表示代理在特定状态下采取特定行动的好坏程度的信号，用于指导代理优化其策略。

一个序列决策过程：智能体与环境进行交互，它在每个时间步(t)都会观察到一个状态（state）$s_t$，然后根据某种策略（policy）选择一个动作（action）$a_t$。执行动作后，环境会根据动作和当前状态转移到新的状态，并提供一个奖励（reward）信号 $r_{t+1}$。

结合基本要素重述强化学习主要目标：找到一种最优策略，即在不同状态下采取的最佳行动，以使累积奖励最大化。这一过程通常通过价值函数来衡量，价值函数评估代理在某个状态下的长期回报。

- **强化学习方法**：基于价值的方法、基于策略的方法以及这两者的结合。
- **DRL**：深度强化学习结合了深度学习和强化学习，通过深度神经网络来学习复杂的策略（实现端到端。
- **状态和观测**：状态是环境的内部表示，而观测是智能体从环境中获取的信息。状态是环境的全貌，观测有时候是全部，有时候是部分。如果环境是完全可观测的，则可以建模为马尔可夫决策过程（MDP）


## 两大方法——基于策略和基于价值

**例子：走迷宫**

假设有一个迷宫，智能体需要学习如何从起始点到达目标点。在每个格子里，智能体可以选择四个动作：向上、向下、向左或向右。每个格子都有一个奖励，目标是找到一种策略，使得智能体在整个迷宫中的总奖励最大。

1. **基于策略的方法：**

   - **策略表示：** 智能体学习一个策略，即在每个格子处选择一个动作的概率分布。例如，对于某个格子，策略可以是\[0.2, 0.4, 0.1, 0.3\]，表示分别选择四个动作的概率。
   
   - **目标：** 优化策略参数，使得整个路径上的动作选择能够最大化累积奖励。

   - **优点：** 可以直接处理连续动作空间，例如，在每个格子处选择动作的概率。

2. **基于价值的方法：**

   - **价值函数表示：** 智能体学习一个价值函数，即在每个格子处采取某个动作后预期获得的累积奖励。例如，对于某个格子和动作，价值函数可以是一个实数。

   - **目标：** 找到最优的值函数，即对于每个状态或状态-动作对，价值函数能够指导智能体做出最优决策。

   - **优点：** 在处理部分可观测问题时可能更为有效。

基于策略和基于价值的强化学习方法是两种不同的方法，它们分别关注于如何表示和学习智能体在环境中做出决策的不同方面。

### 基于策略的强化学习（Policy-Based Reinforcement Learning）：

1. **策略表示：** 基于策略的方法直接学习策略，即给定状态，智能体应该采取的动作的概率分布。策略通常用符号或参数化的形式来表示。

2. **目标：** 目标是找到一个最优的策略，使得在整个任务中累积的期望奖励最大化。优化的焦点是策略的参数，以使得选择的动作在长期内最有可能导致高累积奖励。

3. **优点：** 策略方法适用于高度随机或连续动作空间，而且它们能够直接处理探索-利用的平衡问题。

4. **例子：** 攭者梯度法（Policy Gradient Methods）是基于策略的一类方法。

### 基于价值的强化学习（Value-Based Reinforcement Learning）：

1. **价值函数表示：** 基于价值的方法关注于学习状态或状态-动作对的价值函数，该函数表示在当前状态或状态-动作对上采取行动能够获得的累积奖励。

2. **目标：** 目标是找到最优的值函数，即找到每个状态或状态-动作对的最大累积奖励。策略可以通过使用值函数来推导，例如选择使值函数最大化的动作。

3. **优点：** 价值方法在处理部分可观测问题（部分可观测马尔可夫决策过程，POMDP）时通常更为有效。

4. **例子：** Q-learning 和深度 Q 网络（DQN）是基于价值的方法的例子。

**In value-based methods, rather than learning the policy, we define the policy by hand and we learn a value function. If we have an optimal value function, we will have an optimal policy.
在基于价值的方法中，我们不是学习策略，而是手动定义策略并学习价值函数。如果我们有一个最优价值函数，我们就会有一个最优策略。**
### 区别总结：

With Policy-Based methods, we train the policy directly to learn which action to take given a state.
With value-based methods, we train a value function to learn which state is more valuable and use this value function to take the action that leads to it.

- 基于策略的方法直接学习决策策略，而基于价值的方法学习状态或状态-动作对的价值函数。
- 策略方法更适用于处理连续或高度随机的动作空间，而价值方法在处理部分可观测问题时可能更有效。
- 策略方法直接处理探索-利用的平衡问题，而价值方法通常需要额外的探索策略。

![[Pasted image 20231119202346.png]]
# 两大模型——有模型和免模型
免模型强化学习（Model-Free Reinforcement Learning）和有模型强化学习（Model-Based Reinforcement Learning）的主要区别在于它们处理环境模型的方式以及在学习中是否使用模型。

### 1. **免模型强化学习（Model-Free RL）：**
![[Pasted image 20231117144500.png]]


   - **特点：** 免模型强化学习不依赖于显式的环境模型。智能体（agent）从与环境的交互中学习，通过尝试不同的动作并观察环境的反馈来更新策略或值函数。
   - **方法：** 代表性的方法包括Q-learning、SARSA、深度强化学习（如深度Q网络 - DQN）等。这些方法直接从经验中学习，而**不需要显式地建模环境的动态特性**。

### 2. **有模型强化学习（Model-Based RL）：**
![[Pasted image 20231117144443.png]]
   - **特点：** 有模型强化学习涉及构建对环境的显式模型，并使用该模型来规划未来的行为。这个模型通常包括对状态转移和奖励的估计，即环境的动态。（状态转移概率已知）
   - **方法：** 典型的方法包括动态规划（DP），即基于模型的价值迭代（Model-Based Value Iteration）、基于模型的策略迭代（Model-Based Policy Iteration）等。这些方法使用模型来模拟环境，然后使用规划算法（如值迭代或策略迭代）来找到最优策略。


### 比较总结：
- 在免模型强化学习中，智能体直接从经验中学习，无需事先知道环境的动态。
- 在有模型强化学习中，智能体试图建立一个环境的模型，并使用该模型进行规划，以获得更有效的策略。
- **模型：** 有模型方法关注于学习环境的模型，而免模型方法直接学习策略或价值函数。

![[Pasted image 20231119202241.png]]


# 两个交互——学习与规划

### 学习（Learning）：

学习是指智能体通过与环境的交互，从经验中提取信息、改进策略或价值函数，以提高在未来任务中的性能。

1. **模型学习：** 智能体学习环境的模型，即对环境动态的内部表示。这包括**对状态转移概率和奖励函数的估计**。**有模型学习**通常与规划过程结合使用，使智能体能够通过模拟环境来评估可能的行动。

2. **策略学习或价值函数学习：** 智能体直接学习执行动作的策略或学习状态或状态-动作对的价值函数。这类方法通常涉及使用**免模型学习**算法，例如 **Q-learning、深度 Q 网络（DQN）或策略梯度**等。

### 规划（Planning）：

规划是指智能体使用学到的知识，通过在环境模型或实际环境中模拟未来的状态、动作和奖励，制定最优的行动策略。比如策略迭代或者价值迭代。

1. **模型规划：** 使用学习到的环境模型，智能体通过规划算法来模拟不同动作的影响，然后选择使得长期累积奖励最大化的动作。有模型强化学习方法通常涉及到模型规划。

2. **无模型规划：** 在没有显式环境模型的情况下，智能体通过使用学到的策略或价值函数，使用规划算法来选择最佳的行动。这通常是免模型强化学习方法所采用的方式。

### 学习与规划的关系：

- **相互作用：** 学习和规划通常是相互作用的过程。学到的知识可以用于规划，而规划的结果又可以用于更新学习的模型、策略或价值函数。

- **探索与利用：** 学习阶段通常涉及探索环境以获取更多信息，而规划阶段则侧重于利用学到的知识来做出决策。


# 两个核心——预测和控制
MDP（Markov Decision Process，马尔可夫决策过程）是一种数学框架，用于描述包含状态、动作、奖励和转移概率等元素的决策问题。在MDP中，有两个主要的概念：预测（prediction）和控制（control）。

## 预测（Prediction）：
预测指的是在给定某个策略（policy）下，对MDP的**未来状态和奖励进行估计或预测**。具体来说，预测问题是关于**评估系统在当前策略下的性能有多好的问题**。解决预测问题的方法通常包括**值函数**（value function）的计算，**比如估计状态值函数或者是动作值函数**，其中值函数表示每个状态或状态-动作对的长期累积奖励。

## 控制（Control）：
控制是指在MDP中找到一个最优的策略，使得系统在长期内获得**最大的累积奖励**。控制问题涉及到在每个状态下选择最优的动作，从而达到最优的长期累积奖励。解决控制问题的方法包括价值迭代（value iteration）、策略迭代（policy iteration）等。

## 比较总结：
总体来说，预测解决了“给定一个策略，系统将来会怎样”的问题，而控制解决了“找到一个最优的策略，使得系统在未来获得最大累积奖励”的问题。在强化学习中，MDP提供了一个理论基础，帮助智能体学习如何在一个环境中做出最优的决策。


# 两个策略——离线策略与在线策略
Off-policy vs On-policy
- Off-policy: using **a different policy** for acting (inference) and updating (training).（使用不同的策略来执行（推理）和更新（训练））
	例如，对于 Q-Learning，epsilon 贪婪策略（执行策略）与用于选择最佳下一状态操作值以更新 Q 值（更新策略）的贪婪策略不同。

- On-policy: using the **same policy** for acting and updating.（使用相同的策略进行操作和更新。）
	For instance, with Sarsa, another value-based algorithm, the epsilon-greedy policy selects the next state-action pair, not a greedy policy.（sarsa：epsilon 贪婪策略选择下一个状态-动作对）![[Pasted image 20231126195832.png]]

# RL算法
总体框架图：
![[Pasted image 20231119202141.png]]