# 背景
 之前学习了Advantage Actor Critic (A2C)，这是一种结合了基于价值和基于策略的方法的混合架构，有助于通过减少方差来稳定训练。
	 控制代理行为的 Actor（基于策略的方法）
	 衡量所采取的行动有多好的批评家（基于价值的方法）

近端策略优化(Proximal Policy Optimization (PPO))，这是一种通过**避免太大的策略更新来提高代理训练稳定性**的架构。为此，我们使用一个比率来指示当前政策和旧政策之间的差异，并将该比率限制在特定范围$[1-\epsilon,1+\epsilon]$。 

# PPO
近端策略优化（PPO）的想法是，我们希望通过限制您在每个训练时期对策略所做的更改来提高策略的训练稳定性，希望避免太大的策略更新。

原因：
1. 训练期间较小的策略更新更有可能收敛到最优解决方案。
2. 政策更新的一步太大可能会导致“跌落悬崖”（得到一个糟糕的政策）并需要很长时间甚至没有可能恢复。


所以我们希望能够比较保守的更新策略。所以希望将比率限制在$[1-\epsilon,1+\epsilon]$。

## 截断代理函数

策略梯度损失函数（已经结合了Advantage Learning思想）：
$$L^{PG}(\theta)=E_{t}[\operatorname{log}\pi_{\theta}(a_{t}|s_{t})*A_{t}]$$


想法是，通过对该函数采取梯度上升步骤（相当于对该函数的负值采取梯度下降），推动代理采取导致更高奖励的行动，并避免有害的行动。

然而，问题来自于步长：
	太小，训练过程太慢
	太高，训练中的变化太大


对于 PPO，其想法是使用称为“裁剪代理目标函数”的新目标函数来约束我们的策略更新，该函数将使用裁剪将策略变化限制在小范围内。

引入新函数旨在避免破坏性的大权重更新：
$$\begin{aligned}L^{CLIP}(\theta)&=\hat{\mathbb{E}}_t\Big[\min(r_t(\theta)\hat{A}_t,\operatorname{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t)\Big]\end{aligned}$$

也就是把之前的策略梯度损失函数替换成现在的截断代理函数。
对截断的解释：
![[Pasted image 20231204181005.png]]

疑问：为什么$r_t(\theta)$可以替换掉$log\pi_\theta(a_t \mid s_t)$?
### Visualize

这个裁剪后的代理目标函数是什么样子？见下文：

![[Pasted image 20231204181304.png]]
表格来自丹尼尔·比克（Daniel Bick）的“对近端策略优化提供连贯的独立解释”

$A_t$ >0 表示采取这个动作更好，所以是希望增加在该状态下采取该操作的概率。

我们有六种不同的情况。首先请记住，我们取裁剪目标和未裁剪目标之间的最小值。
- ①和②都是在范围之内，所以不用裁剪。第一种$A_t>0$ 所以该动作优于该状态下所有动作的平均值。所以最后是鼓励这个动作。第二种小于0，所以降低这个动作的概率。
- ③和④都是小于这个边界比例。说明这个动作在这个策略下发生的概率较小于在旧的策略框架下。对于③来说，$A_t$ 显示希望增加在该状态下采取该操作的概率，所以增加。对于④来说，优势估计为负，所以我们应该要降低这个动作的概率，但是由于低于下限，我们不想进一步降低，所以直接导数为0.
- ⑤和⑥都是大于这个边界比例。对于⑤来说，advantage是正的，说明应该增加这个比例，但是已经超过上限了，所以不行，所以我们梯度还是为0。对于⑥的话，advantage是负的，说明这个动作不好，我们应该降低这个比例，所以往下降没有关系。


为什么梯度为0？因为求导求的是关于 $\theta$ 的导数呀，梯度截断了之后，就和 $\theta$ 没关系了。所以导数就为0。



