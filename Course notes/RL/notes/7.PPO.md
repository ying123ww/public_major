# 背景
 之前学习了Advantage Actor Critic (A2C)，这是一种结合了基于价值和基于策略的方法的混合架构，有助于通过减少方差来稳定训练。
	 控制代理行为的 Actor（基于策略的方法）
	 衡量所采取的行动有多好的批评家（基于价值的方法）

近端策略优化(Proximal Policy Optimization (PPO))，这是一种通过**避免太大的策略更新来提高代理训练稳定性**的架构。为此，我们使用一个比率来指示当前政策和旧政策之间的差异，并将该比率限制在特定范围$[1-\epsilon,1+\epsilon]$。 旨在解决其他策略梯度方法（如TRPO，Trust Region Policy Optimization）中的一些难题，同时保持计算效率和易于实现的特点。

PPO算法的主要原理可以概括为以下几点：

1. **概率比率裁剪（Clipped Probability Ratios）**：PPO引入了一个概率比率的概念，该比率是新策略和旧策略下采取同一动作的概率之比。PPO通过裁剪这个比率的范围，来限制策略更新的幅度。这意味着它会惩罚偏离旧策略太多的更新，以此来防止策略更新过快。

2. **目标函数**：PPO算法设计了一种特殊的目标函数，这个函数不仅关注于最大化预期回报，还考虑了策略更新的幅度。目标函数通常包含两个部分：一部分是期望奖励的估计，另一部分则是策略比率的裁剪函数。

3. **多步优化**：与其他策略梯度方法相比，PPO的另一个优势是可以在同一组样本上执行多次梯度更新。这减少了样本的需求量，提高了学习效率。

4. **稳定与高效**：由于策略更新步幅的限制，PPO算法在训练过程中通常更稳定，且在多种强化学习任务中都显示出了较高的效率和效果。

# PPO
近端策略优化（PPO）的想法是，我们希望通过限制您在每个训练时期对策略所做的更改来提高策略的训练稳定性，希望避免太大的策略更新。

原因：
1. 训练期间较小的策略更新更有可能收敛到最优解决方案。
2. 政策更新的一步太大可能会导致“跌落悬崖”（得到一个糟糕的政策）并需要很长时间甚至没有可能恢复。


所以我们希望能够比较保守的更新策略。所以希望将比率限制在$[1-\epsilon,1+\epsilon]$。

## 截断代理函数

策略梯度损失函数（已经结合了Advantage Learning思想）：
$$L^{PG}(\theta)=E_{t}[\operatorname{log}\pi_{\theta}(a_{t}|s_{t})*A_{t}]$$


想法是，通过对该函数采取梯度上升步骤（相当于对该函数的负值采取梯度下降），推动代理采取导致更高奖励的行动，并避免有害的行动。

然而，问题来自于步长：
	太小，训练过程太慢
	太高，训练中的变化太大


对于 PPO，其想法是使用称为“裁剪代理目标函数”的新目标函数来约束我们的策略更新，该函数将使用裁剪将策略变化限制在小范围内。

引入新函数旨在避免破坏性的大权重更新：
$$\begin{aligned}L^{CLIP}(\theta)&=\hat{\mathbb{E}}_t\Big[\min(r_t(\theta)\hat{A}_t,\operatorname{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t)\Big]\end{aligned}$$

也就是把之前的策略梯度损失函数替换成现在的截断代理函数。
对截断的解释：
![[Pasted image 20231204181005.png]]

疑问：为什么$r_t(\theta)$可以替换掉$log\pi_\theta(a_t \mid s_t)$?
### Visualize

这个裁剪后的代理目标函数是什么样子？见下文：

![[Pasted image 20231204181304.png]]
表格来自丹尼尔·比克（Daniel Bick）的“对近端策略优化提供连贯的独立解释”

$A_t$ >0 表示采取这个动作更好，所以是希望增加在该状态下采取该操作的概率。

我们有六种不同的情况。首先请记住，我们取裁剪目标和未裁剪目标之间的最小值。
- ①和②都是在范围之内，所以不用裁剪。第一种$A_t>0$ 所以该动作优于该状态下所有动作的平均值。所以最后是鼓励这个动作。第二种小于0，所以降低这个动作的概率。
- ③和④都是小于这个边界比例。说明这个动作在这个策略下发生的概率较小于在旧的策略框架下。对于③来说，$A_t$ 显示希望增加在该状态下采取该操作的概率，所以增加。对于④来说，优势估计为负，所以我们应该要降低这个动作的概率，但是由于低于下限，我们不想进一步降低，所以直接导数为0.
- ⑤和⑥都是大于这个边界比例。对于⑤来说，advantage是正的，说明应该增加这个比例，但是已经超过上限了，所以不行，所以我们梯度还是为0。对于⑥的话，advantage是负的，说明这个动作不好，我们应该降低这个比例，所以往下降没有关系。


为什么梯度为0？因为求导求的是关于 $\theta$ 的导数呀，梯度截断了之后，就和 $\theta$ 没关系了。所以导数就为0。

# 代码
```python
import gym
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import rl_utils


class PolicyNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return F.softmax(self.fc2(x), dim=1)


class ValueNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim):
        super(ValueNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)


class PPO:
    ''' PPO算法,采用截断方式 '''
    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,
                 lmbda, epochs, eps, gamma, device):
        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)
        self.critic = ValueNet(state_dim, hidden_dim).to(device)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),
                                                lr=actor_lr)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),
                                                 lr=critic_lr)
        self.gamma = gamma
        self.lmbda = lmbda
        self.epochs = epochs  # 一条序列的数据用来训练轮数
        self.eps = eps  # PPO中截断范围的参数
        self.device = device

    def take_action(self, state):
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        probs = self.actor(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item()

    def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'],
                              dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(
            self.device)
        rewards = torch.tensor(transition_dict['rewards'],
                               dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(transition_dict['next_states'],
                                   dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'],
                             dtype=torch.float).view(-1, 1).to(self.device)
        td_target = rewards + self.gamma * self.critic(next_states) * (1 -
                                                                       dones)
        td_delta = td_target - self.critic(states)
        advantage = rl_utils.compute_advantage(self.gamma, self.lmbda,
                                               td_delta.cpu()).to(self.device)
        old_log_probs = torch.log(self.actor(states).gather(1,
                                                            actions)).detach()

        for _ in range(self.epochs):
            log_probs = torch.log(self.actor(states).gather(1, actions))
            ratio = torch.exp(log_probs - old_log_probs)
            surr1 = ratio * advantage
            surr2 = torch.clamp(ratio, 1 - self.eps,
                                1 + self.eps) * advantage  # 截断
            actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPO损失函数
            critic_loss = torch.mean(
                F.mse_loss(self.critic(states), td_target.detach()))
            self.actor_optimizer.zero_grad()
            self.critic_optimizer.zero_grad()
            actor_loss.backward()
            critic_loss.backward()
            self.actor_optimizer.step()
            self.critic_optimizer.step()
```
## 解释

### 框架
这段代码实现了PPO (Proximal Policy Optimization) 算法，一个在强化学习中常用的策略优化算法。代码主要由三个类组成：`PolicyNet`（策略网络），`ValueNet`（价值网络），以及`PPO`（实现PPO算法的主体）。以下是各个部分的详细解释：

#### 1. 导入库

- **gym**：用于创建和管理强化学习环境。
- **torch**：PyTorch，一个主流的深度学习库。
- **torch.nn.functional**：提供了一系列用于构建神经网络的函数。
- **numpy**：数学计算库。
- **matplotlib.pyplot**：绘图库。
- **rl_utils**：可能是一个自定义库，用于实现强化学习中的一些实用功能。

#### 2. PolicyNet 类

定义了一个策略网络，用于生成动作概率分布。它有以下组件：

- **构造函数**：定义了两个全连接层，用于处理输入状态。
- **forward 方法**：定义了网络的前向传播逻辑，使用ReLU激活函数和Softmax输出动作概率。

#### 3. ValueNet 类

定义了一个价值网络，用于估计状态的价值。它也有两个全连接层和一个前向传播逻辑。

#### 4. PPO 类

实现了PPO算法，包含以下关键方法和属性：

- **构造函数**：初始化策略网络（actor）、价值网络（critic）以及各自的优化器。还设置了几个PPO算法的关键参数。
- **take_action 方法**：根据当前状态选择动作。
- **update 方法**：更新策略网络和价值网络。使用的关键步骤包括：
  - 计算TD（Temporal Difference）目标和优势。
  - 使用概率比率和裁剪技术计算PPO的目标函数。
  - 同时更新策略网络和价值网络。

#### 代码框架描述

1. **初始化**：
   - 创建策略网络和价值网络。
   - 设置优化器和一些算法参数。

2. **动作选择**：
   - 根据当前状态，使用策略网络生成动作。

3. **学习更新**：
   - 收集一系列的转换（状态、动作、奖励等）。
   - 计算TD目标和优势函数。
   - 使用PPO特定的损失函数更新策略网络和价值网络。

这段代码提供了PPO算法的一个基础实现框架，适用于解决强化学习问题。代码中可能缺少一些细节，如环境交互、数据收集、`rl_utils.compute_advantage`函数的实现等。


### 计算TD目标和优势函数

TD目标和优势函数的计算发生在 `update` 方法的以下部分：

```python
td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)
td_delta = td_target - self.critic(states)
advantage = rl_utils.compute_advantage(self.gamma, self.lmbda, td_delta.cpu()).to(self.device)
```

- `td_target` 计算了TD目标，它是根据即时奖励、折扣因子 (`self.gamma`)，以及下一个状态的价值估计值（由价值网络 `self.critic` 提供）得到的。
- `td_delta` 是TD误差，即TD目标和当前状态的价值估计值之间的差异。
- `advantage` 则是利用TD误差计算的优势函数。这里使用了 `rl_utils.compute_advantage` 函数，这个函数可能是用于计算基于GAE（Generalized Advantage Estimation）的优势函数。

### 使用概率比率和裁剪技术计算PPO的目标函数

在PPO（Proximal Policy Optimization）算法的实现中，"截断"（clipping）是一个关键特性，主要体现在更新策略网络时计算损失函数的部分。在您提供的代码中，这一部分体现在 `PPO` 类的 `update` 方法中。

以下是截断体现的具体代码段：

```python
for _ in range(self.epochs):
    log_probs = torch.log(self.actor(states).gather(1, actions))
    ratio = torch.exp(log_probs - old_log_probs)  # 概率比率
    surr1 = ratio * advantage
    surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage  # 截断
    actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPO损失函数
    ...
```

在这段代码中，截断发生在计算 `surr2` 的部分。关键步骤包括：

1. **计算概率比率（ratio）**：比率是新策略与旧策略产生动作的概率之比。

2. **应用截断**：通过 `torch.clamp` 函数，概率比率被限制在 `[1 - self.eps, 1 + self.eps]` 的范围内。这里的 `self.eps` 是PPO算法的一个超参数，用于控制截断的幅度。

3. **计算损失函数**：损失函数使用未截断的概率比率（surr1）和截断后的概率比率（surr2）的较小值，这是PPO特有的损失函数形式。

截断操作的目的是减少策略更新的幅度，防止由于大的策略更新导致的性能不稳定。通过限制概率比率的变化范围，PPO能够保证策略的更新既不过于保守，也不过于激进，从而在提高学习效率的同时保持一定的稳定性。

### 同时更新策略网络和价值网络

在同一个循环中，代码同时对策略网络和价值网络进行了更新：

```python
    ...
    actor_loss.backward()
    critic_loss.backward()
    self.actor_optimizer.step()
    self.critic_optimizer.step()
```

- 首先计算 `actor_loss` 和 `critic_loss` 的梯度（`.backward()`）。
- 然后分别使用优化器 `self.actor_optimizer` 和 `self.critic_optimizer` 更新策略网络和价值网络的参数。

这些步骤实现了PPO算法中的核心机制，包括TD目标的计算、优势函数的应用、概率比率的裁剪，以及策略网络和价值网络的联合优化。这些机制共同确保了PPO算法在学习过程中的高效性和稳定性。

# Q&A
##  那么截断主要是针对策略网络的吗

是的，截断（clipping）机制在PPO（Proximal Policy Optimization）算法中主要针对策略网络（Actor）。这一机制的核心目的是为了限制策略更新的幅度，以此确保学习过程的稳定性。

在PPO算法的实现中，截断体现在如何计算策略损失函数。具体来说，这涉及到以下几个步骤：

1. **概率比率**：计算新策略和旧策略下采取相同动作的概率比率。

2. **优势函数**：使用优势函数来估计特定动作相比平均动作的额外价值。

3. **应用截断**：在计算策略损失时，对概率比率进行裁剪。如果概率比率超出了设定的区间（通常是 1 - ε, 1 + ε]），则将其裁剪到这个区间内。

4. **最小化损失**：PPO损失函数取裁剪后和未裁剪概率比率的优势函数乘积的较小者，这样确保了策略更新步幅的合理性。

通过这种方式，PPO能够避免在策略更新时出现过大的变化，从而防止算法性能的突然下降。这是一种有效平衡探索和利用，保持学习稳定性的方法。而价值网络（Critic）的更新则主要是基于TD目标，用于估计状态或状态-动作对的价值，与截断机制无关。

## TD和优势函数在这里的作用

是什么？

### TD（Temporal Difference）目标

TD目标是在基于值的强化学习方法中用于估计未来奖励的关键概念。在这个上下文中，TD目标通常指的是估计的回报，它结合了即时奖励和对未来状态价值的预测。TD目标的计算公式通常如下：

\[ \text{TD目标} = r_t + \gamma \cdot V(s_{t+1}) \]

其中，\( r_t \) 是在时间步 \( t \) 获得的即时奖励，\( \gamma \) 是折扣因子，\( V(s_{t+1}) \) 是对下一状态 \( s_{t+1} \) 的价值预测。

TD目标用于计算TD误差，即当前价值估计和TD目标之间的差异，这个误差被用于更新价值函数。

### 优势函数（Advantage Function）

优势函数是Actor-Critic方法中的一个概念，用于评估采取特定动作相比于平均情况的优越性。优势函数的计算通常如下：

\[ A(s, a) = Q(s, a) - V(s) \]

或者使用TD误差的形式：

\[ A(s, a) = r_t + \gamma \cdot V(s_{t+1}) - V(s_t) \]

这里，\( A(s, a) \) 是状态 \( s \) 下采取动作 \( a \) 的优势值，\( Q(s, a) \) 是采取动作 \( a \) 在状态 \( s \) 下的动作价值，而 \( V(s) \) 是状态 \( s \) 的价值。

优势函数提供了一个相对于平均行为的性能指标，使得算法能够区分在特定状态下哪些动作比平均情况更好。

### 作用
在PPO算法中，计算TD目标和优势函数是策略更新过程的关键步骤。这些计算帮助算法在训练过程中更有效地优化策略，并保持学习的稳定性。

在PPO算法中，计算TD（Temporal Difference）目标和优势函数是用来同时更新策略网络（Actor）和价值网络（Critic）。这两个步骤在不同的方面支持网络的更新：

1. **更新价值网络**：
   - TD目标的计算直接用于更新价值网络。
   - 价值网络的目的是准确估计状态的价值，所以使用TD目标作为训练目标来训练价值网络。
   - TD误差（即TD目标与当前价值估计的差异）是价值网络更新的关键信号。

2. **更新策略网络**：
   - 优势函数的计算则是为了更新策略网络。
   - 优势函数衡量了在给定状态下，特定动作相比于平均动作的额外价值。这个信息对于策略网络来说非常重要，因为它可以告诉网络哪些动作比平均情况更好。
   - 在PPO中，优势函数用于计算策略梯度和进行策略更新，特别是在计算概率比率和裁剪技术应用于PPO损失函数时。

因此，TD目标和优势函数在PPO算法中起到了双重作用：一方面指导价值网络的更新，另一方面帮助优化策略网络。通过这种方式，PPO能够有效地平衡探索和利用，同时保持策略的稳定性和有效性。


## 离散的PPO和连续的PPO
在强化学习中，处理连续动作空间和处理离散动作空间的策略网络有显著的不同。您提供的代码展示了一个为连续动作空间设计的PPO算法实现，我将详细解释与离散动作空间PPO实现的不同之处：

### 连续动作空间的PPO（`PolicyNetContinuous` 和 `PPOContinuous`）

1. **策略网络输出**：连续动作空间的策略网络（`PolicyNetContinuous`）输出一个动作的均值（`mu`）和标准差（`std`）。这是因为在连续动作空间中，策略通常表示为一个概率分布，最常见的是正态分布。

2. **动作生成**：基于策略网络输出的均值和标准差，生成动作时使用正态分布（`torch.distributions.Normal`）。这与离散动作空间中使用分类分布（如多项式分布）不同。

3. **损失函数计算**：损失函数的计算涉及到对数概率（`log_prob`）的计算，它基于连续分布而非离散分布。

### 离散动作空间的PPO

1. **策略网络输出**：在离散动作空间中，策略网络通常输出每个动作的概率分布。

2. **动作生成**：基于策略网络输出的概率分布，通常使用分类分布（如Categorical分布）来采样动作。

3. **损失函数计算**：在离散动作空间中，损失函数的计算也会涉及概率比率和优势函数，但是这些是基于离散的动作概率计算的。

总的来说，主要的区别在于策略网络的输出和动作采样方法。连续动作空间的PPO使用了正态分布来建模动作，而离散动作空间的PPO则使用分类分布。这导致了动作采样方法和损失函数计算在两种情况下有所不同。连续动作空间通常用于更复杂的环境，如机器人控制，其中动作可以在一个范围内取任何值；而离散动作空间适用于动作选项是固定、离散的情况，如棋类游戏。

当然，让我们来对比和区分您提供的连续动作空间PPO代码和一个典型的离散动作空间PPO实现代码的主要不同点。

### 连续动作空间PPO代码特点

1. **策略网络输出**：连续动作空间的`PolicyNetContinuous`输出动作的均值（`mu`）和标准差（`std`），这对应于连续动作空间中的动作概率分布。

    ```python
    mu = 2.0 * torch.tanh(self.fc_mu(x))
    std = F.softplus(self.fc_std(x))
    ```

2. **动作采样方式**：使用正态分布（`torch.distributions.Normal`）进行动作采样。

    ```python
    action_dist = torch.distributions.Normal(mu, sigma)
    action = action_dist.sample()
    ```

3. **损失函数计算**：使用连续概率分布的对数概率进行损失计算。

    ```python
    action_dists = torch.distributions.Normal(mu, std)
    log_probs = action_dists.log_prob(actions)
    ```

### 离散动作空间PPO代码特点（假设实现）

1. **策略网络输出**：离散动作空间的策略网络通常只有一个输出层，输出每个动作的概率，使用softmax函数。

    ```python
    def forward(self, x):
        x = F.relu(self.fc1(x))
        return F.softmax(self.fc2(x), dim=1)
    ```

2. **动作采样方式**：使用分类分布（如`torch.distributions.Categorical`）进行动作采样。

    ```python
    probs = self.actor(state)
    action_dist = torch.distributions.Categorical(probs)
    action = action_dist.sample()
    ```

3. **损失函数计算**：使用离散概率分布的对数概率进行损失计算。

    ```python
    log_probs = torch.log(self.actor(states).gather(1, actions))
    ```

### 主要区别

- **动作空间类型**：连续动作空间的实现关注于生成一个连续的动作值，通常是动作的均值和标准差；而离散动作空间关注于从有限的动作集合中选择一个动作。
- **策略网络结构和输出**：连续动作空间的策略网络输出均值和标准差，而离散动作空间的策略网络输出每个动作的概率。
- **动作采样方法**：连续动作空间使用正态分布采样，离散动作空间使用分类分布采样。
- **损失函数的计算**：这两种情况下损失函数的计算方式不同，反映了动作空间的不同。

这些区别体现了连续动作空间和离散动作空间在策略表示和优化策略时的不同需求和挑战。