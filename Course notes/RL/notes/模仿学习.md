前提：奖励函数十分重要，奖励函数的微调会带来巨大的影响。
假设存在一个专家智能体，其策略可以看成最优策略，我们就可以直接模仿这个专家在环境中交互的状态动作数据来训练一个策略，并且不需要用到环境提供的奖励信号。

![[IMG-20240103204008088.png]]
1. **行为克隆（Behavior Cloning, BC）**：这是一种简单的模仿学习方法，其中代理直接从专家的行为数据中学习，通过观察和复制这些行为来执行任务，类似于学生通过模仿老师的示例来学习。

2. **逆强化学习（Inverse Reinforcement Learning, IRL）**：这是一种从观察到的行为中推断出未知奖励函数的方法，它试图理解为什么专家会以特定的方式行动，而不仅仅是复制这些行动。

3. **生成式对抗模仿学习（Generative Adversarial Imitation Learning, GAIL）**：结合了生成式对抗网络（GANs）和模仿学习的技术，其中一个网络生成行为，另一个网络评估这些行为是否类似于专家的行为，通过这种对抗过程，代理学习模仿专家的行为。

# 行为克隆

直接使用监督学习的方法，将专家数据中的$(s_1,a_1)$ 中的$s_1$看作样本输入，$a_1$看作标签。目标为：

$$\theta^*=\arg\min_\theta\mathbb{E}_{(s,a)\sim B}[\mathcal{L}(\pi_\theta(s),a)]$$
其中，B是专家的数据集，L是对应监督学习框架下的损失函数。若动作是离散的，该损失函数可以是最大似然估计得到的。若动作是连续的，该损失函数可以是均方误差函数。

需要数据量较大，作为策略预训练，较为快速的提高机器策略行为的一种方法。
若数据量较小，则会带来复合误差的问题。（遇到专家数据没有见过的状态，就会随机选择一个策略，导致偏差越来越大）
![[IMG-20240103204640734.png]]
# 生成对抗模仿学习
GAIL是受到了生成式对抗网络（Generative Adversarial Networks，简称GANs）的启发。在GANs中，有两个部分：一个生成器（尝试创建看起来真实的数据）和一个鉴别器（尝试区分真实数据和生成器产生的数据）。这两个网络相互竞争，不断提高各自的性能。
GAIL将这种思想应用到模仿学习中。在GAIL中，我们有一个生成器，这里是模仿专家行为的代理；还有一个鉴别器，它的任务是区分代理的行为和专家的行为。代理（生成器）尝试以一种让鉴别器难以区分它和专家行为的方式行动，而鉴别器则尝试变得更擅长区分这两者。通过这种对抗过程，代理逐渐学会模仿专家的行为。

**相当于我们也训练一个PPO算法，但是PPO算法需要与环境交互（才能得到reward）。但是现在我们只用专家数据，而不与环境交互。我们用专家数据和自己生成的数据放入辨别器中得到reward，用这个reward去更新PPO算法，使得PPO算法中策略网络生成的策略与专家数据此状态生成的策略越来越近。**
 
### 定义和背景

GAIL结合了生成式对抗网络（GAN）的思想与模仿学习。在GAIL框架中，模仿学习被公式化为一个最小-最大优化问题，其中包括一个生成模型（代理）和一个鉴别模型（鉴别器）。

- **生成模型（代理）**：试图模仿专家的策略，生成行为序列。
- **鉴别模型（鉴别器）**：区分代理生成的行为和专家行为。

### 核心公式

GAIL的目标是最小化以下目标函数：

$$ \min_{\theta} \max_{\omega} \mathbb{E}_{\pi_{\theta}}[\log(D_{\omega}(s,a))] + \mathbb{E}_{\pi_{E}}[\log(1 - D_{\omega}(s,a))] - \lambda H(\pi_{\theta}) $$

其中：

- $\theta$ 表示代理的策略参数。
- $\omega$ 表示鉴别器的参数。
- $\pi_{\theta}$ 是代理的策略，即生成模型。
- $\pi_{E}$ 是专家的策略。
- $D_{\omega}(s,a)$ 是鉴别器的输出，表示在状态 $s$ 下，行动 $a$ 来自专家的概率。
- $H(\pi_{\theta})$ 是代理策略的熵，用于鼓励探索。
- $\lambda$ 是一个正则化项的权重，控制熵的重要性。

### 工作流程

1. **训练鉴别器**：使用专家数据和代理生成的数据来训练鉴别器。目标是最大化能够区分来自代理和专家的行为。
2. **优化代理策略**：通过强化学习方法（如策略梯度）来优化代理的策略，使其行为更难被鉴别器区分，即生成类似于专家的行为。
3. **迭代过程**：交替进行鉴别器的训练和代理策略的优化，直到代理生成的行为与专家行为足够相似，使鉴别器难以区分。

### 关键点

- **模仿专家行为**：GAIL不直接从专家行为中学习，而是通过对抗过程来学习生成类似专家的行为。
- **平衡探索和模仿**：通过熵正则化，确保代理在模仿的同时保持一定的探索性。
- **强化学习和GAN的结合**：利用GAN的框架在模仿学习中引入了对抗性训练，这是GAIL的核心创新。



# 代码
## 行为克隆
在行为克隆（Behavior Cloning, BC）中使用最大似然估计（Maximum Likelihood Estimation, MLE）来计算损失是因为BC实质上是一种监督学习问题。在这种情况下，我们有一个由专家行为提供的数据集（状态-动作对），目标是训练一个模型（在这种情况下是策略网络）来模仿这些行为。下面详细解释这些概念：

### 最大似然估计计算损失
1. **最大似然估计的目的**：MLE的目的是找到使得观测数据出现概率最大的模型参数。在行为克隆的上下文中，这意味着找到一组网络参数，使得在这些参数下，观测到专家执行的动作的概率最大。

2. **计算方法**：在实践中，这通常通过最小化负对数似然（Negative Log Likelihood, NLL）来实现。在这个代码中，`log_probs`计算了策略网络对于给定状态输出特定动作的对数概率。通过最小化这个负对数似然（即最大化对数似然），网络被训练来增加选择专家动作的概率。

### 训练过程
1. **训练的核心**：训练过程涉及到在策略网络上使用监督学习方法。每一步训练都包括向网络提供一批状态，并使网络学习产生与这些状态对应的专家动作。

2. **迭代的必要性**：代码中确实进行了迭代。在主训练循环中（`for i in range(n_iterations)`），代理在每次迭代中都会学习专家数据的一个新批次。虽然这不是传统意义上的迭代改进（如在强化学习中的策略迭代或值迭代），但每次迭代都旨在进一步优化策略网络的参数。

### 与交叉熵损失的比较
1. **损失函数**：在分类问题中，交叉熵损失是非常常见的损失函数。在某种意义上，负对数似然与交叉熵损失非常相似，实际上在某些情况下它们是等价的。

2. **应用场景**：交叉熵损失通常用于多分类问题，它衡量的是模型预测的概率分布和真实标签的概率分布之间的差异。在行为克隆中，尽管我们的目标是模仿动作，但这本质上是一个预测问题，其中模型需要预测在给定状态下执行的动作。

3. **最大似然与交叉熵的关系**：在处理离散动作空间时，最大似然估计的目标是最大化模型选择实际采取的动作的概率，这与最小化交叉熵损失的目标非常相似。事实上，当使用softmax作为输出层并计算分类任务的对数似然时，这等同于计算交叉熵损失。






## 生成对抗模仿学习


```python
class Discriminator(nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(Discriminator, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x, a):
        cat = torch.cat([x, a], dim=1)
        x = F.relu(self.fc1(cat))
        return torch.sigmoid(self.fc2(x))
class GAIL:
    def __init__(self, agent, state_dim, action_dim, hidden_dim, lr_d):
        self.discriminator = Discriminator(state_dim, hidden_dim,
                                           action_dim).to(device)
        self.discriminator_optimizer = torch.optim.Adam(
            self.discriminator.parameters(), lr=lr_d)
        self.agent = agent

    def learn(self, expert_s, expert_a, agent_s, agent_a, next_s, dones):
        expert_states = torch.tensor(expert_s, dtype=torch.float).to(device)
        expert_actions = torch.tensor(expert_a).to(device)
        agent_states = torch.tensor(agent_s, dtype=torch.float).to(device)
        agent_actions = torch.tensor(agent_a).to(device)
        expert_actions = F.one_hot(expert_actions, num_classes=2).float()
        agent_actions = F.one_hot(agent_actions, num_classes=2).float()

        expert_prob = self.discriminator(expert_states, expert_actions)
        agent_prob = self.discriminator(agent_states, agent_actions)
        discriminator_loss = nn.BCELoss()(
            agent_prob, torch.ones_like(agent_prob)) + nn.BCELoss()(
                expert_prob, torch.zeros_like(expert_prob))
        self.discriminator_optimizer.zero_grad()
        discriminator_loss.backward()
        self.discriminator_optimizer.step()

        rewards = -torch.log(agent_prob).detach().cpu().numpy()
        transition_dict = {
            'states': agent_s,
            'actions': agent_a,
            'rewards': rewards,
            'next_states': next_s,
            'dones': dones
        }
        self.agent.update(transition_dict)


env.seed(0)
torch.manual_seed(0)
lr_d = 1e-3
agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda,
            epochs, eps, gamma, device)
gail = GAIL(agent, state_dim, action_dim, hidden_dim, lr_d)
n_episode = 500
return_list = []

with tqdm(total=n_episode, desc="进度条") as pbar:
    for i in range(n_episode):
        episode_return = 0
        state = env.reset()
        done = False
        state_list = []
        action_list = []
        next_state_list = []
        done_list = []
        while not done:
            action = agent.take_action(state)
            next_state, reward, done, _ = env.step(action)
            state_list.append(state)
            action_list.append(action)
            next_state_list.append(next_state)
            done_list.append(done)
            state = next_state
            episode_return += reward
        return_list.append(episode_return)
        gail.learn(expert_s, expert_a, state_list, action_list,
                   next_state_list, done_list)
        if (i + 1) % 10 == 0:
            pbar.set_postfix({'return': '%.3f' % np.mean(return_list[-10:])})
        pbar.update(1)

# 进度条: 100%|██████████| 500/500 [04:08<00:00,  2.01it/s, return=200.000]
```


这段代码实现了一个名为GAIL（Generative Adversarial Imitation Learning）的深度学习模型，用于强化学习环境中。它包括两个主要部分：判别器（Discriminator）和GAIL类本身。

1. **判别器（Discriminator）类**：
   - **初始化** (`__init__`): 这个类继承自PyTorch的`nn.Module`。它初始化两个全连接层（`fc1`和`fc2`）。`fc1`将状态（state）和动作（action）的维度合并，并输出到隐藏层维度。`fc2`将隐藏层的输出转换为一个标量值。
   - **前向传播** (`forward`): 这个方法接收状态和动作作为输入，将它们拼接（`torch.cat`），通过第一个全连接层并应用ReLU激活函数，然后通过第二个全连接层并应用sigmoid激活函数。这个输出代表给定状态和动作是由专家（expert）还是由代理（agent）生成的概率。

2. **GAIL类**：
   - **初始化** (`__init__`): 这个类初始化了一个判别器实例，并为其创建一个优化器（使用Adam优化算法）。它还存储了一个代理（agent），这是一个进行学习的强化学习算法（在代码中为PPO算法）。
   - **学习** (`learn`): 这个方法接收专家和代理的状态和动作，将它们转换为张量，并通过判别器计算概率。然后，它计算判别器的损失（使用二元交叉熵损失），执行反向传播并更新判别器的权重。最后，它计算代理的奖励并更新代理的状态。

3. **训练循环**：
   - 这部分代码设置了训练的环境和参数，创建了PPO代理和GAIL实例。
   - 它通过一系列环境交互（`env.step(action)`）进行多个训练周期。在每个周期中，它收集状态、动作和奖励，然后使用这些数据调用`gail.learn`来更新判别器和代理。
   - 训练进度和平均回报（average return）通过tqdm库的进度条显示。

整体来看，这段代码实现了GAIL算法，用于在强化学习中模仿专家的行为。通过对抗性训练，它能够训练一个代理以模仿专家的策略。



当然，让我们深入了解一下GAIL类的具体工作原理，并比较它与传统的PPO算法的不同之处。

### GAIL类的工作原理

GAIL（Generative Adversarial Imitation Learning）是一种结合了生成对抗网络（GAN）原理和强化学习的算法。在这个类中，主要的工作可以分为以下几个部分：

1. **初始化（`__init__`）**:
   - 创建了一个判别器实例，该判别器用于区分代理生成的状态-动作对和专家生成的状态-动作对。
   - 初始化了一个优化器，用于优化判别器的参数。
   - 存储了一个代理（例如PPO），这是用于在环境中做决策的强化学习算法。

2. **学习过程（`learn`）**:
   - **数据准备**：将专家和代理的状态和动作转换为张量，并进行必要的处理（如one-hot编码）。
   - **判别器训练**：使用专家和代理的数据，通过判别器来评估这些数据是由专家还是代理生成的概率。接着计算判别器的损失（使用二元交叉熵），并通过反向传播更新判别器的参数。
   - **奖励计算**：根据判别器输出的概率计算代理的奖励。如果一个状态-动作对更像是由专家生成的，那么它会得到更低的奖励。
   - **代理更新**：使用计算得到的奖励和收集到的状态-动作对，调用代理（如PPO）的更新方法来改善其策略。

### GAIL与PPO的区别

1. **目标不同**:
   - **PPO** 是一种策略梯度算法，旨在直接通过最大化预期奖励来改善其策略。
   - **GAIL** 则是通过模仿学习，其目标是学习一个策略，使得代理的行为尽可能接近专家的行为。

2. **奖励信号的来源**:
   - 在**PPO**中，奖励信号直接来自环境。
   - 在**GAIL**中，奖励是由判别器基于状态-动作对的“专家程度”动态生成的。

3. **学习机制**:
   - **PPO**通过优化策略来直接提高在给定任务上的表现。
   - **GAIL**则使用了生成对抗的框架，其中判别器和生成器（代理）相互竞争：判别器学习区分专家和代理的行为，而代理则试图欺骗判别器，使其行为看起来更像专家的。

总结来说，GAIL类通过结合判别器的对抗性训练和代理的策略更新，实现了一种高效的模仿学习方法。这与传统的PPO算法在目标、奖励信号来源和学习机制上有显著的不同。