# 背景
前文我们接触到基于策略的算法，Reinforce，也就是蒙特卡洛强化。
然后再基于策略的方法中，我们的目标是直接优化策略，而不是涉及到价值函数。Reinforce是基于策略方法的子类（策略梯度）中的一部分。策略梯度通过使用梯度上升来优化权重。

但是前文的蒙特卡洛采样会有一些麻烦，会带来方差问题（见下文）

今天我们用Actor-Critic方法——结合了价值和策略。
	1. 有一个actor控制代理行为（基于策略的方法）
	2. 衡量所采取行动的好坏批评家（基于价值的方法）


# Variance in Reinforce
强化学习的方差问题

我们希望按照回报的高低成比例的增加动作中的概率。
![[Pasted image 20231202142819.png]]


这里的$R(\tau)$ 是采用蒙特卡洛采样得到的。
$$R(\tau)=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+...$$
这种方法的优点是，它是无偏见（no unbiased）的。但是由于环境的随机性，轨迹可能会导致不同的回报，导致高方差。所以相同的起始状态可能会导致截然不同的回报。
我们想的可能是那么我们通过使用大量轨迹来减轻方差，希望任何一条轨迹中引入的方差能够总体减少并提供“真实”的回报估计。但是增加批量大小会减少效率。

# Actor-Critic 
所以最后提出了用Actor-Critic方差来解决高方差的问题。
我们学习两个近似：
- Agent：控制代理行为的策略函数，$\pi_\theta(s)$
- Critic：通过衡量所采取的行动的好坏来协助策略更新的值函数，$\hat{q}_w(s,a)$

总体步骤如下：
![[Pasted image 20231204141936.png]]


## A2C——引入advantage函数
在更新Critic的函数的时候，可以通过使用 Advantage 函数而不是 Action 值函数。

这个想法是，advantage函数计算一个动作与某个状态下可能的其他动作相比的相对优势：**与该状态的平均值相比**，在某个状态下采取该动作如何**更好**。它从状态动作对中减去状态的平均值：
![[Pasted image 20231204143615.png]]

可以看作是额外的奖励，即超出该状态的预期值的奖励。
在更新的时候：
	如果 A(s,a) > 0：我们的梯度被推向那个方向。
	如果 A(s,a) < 0（我们的动作比该状态的平均值更差），我们的梯度就会向相反的方向推动。



通过引入优势函数，可以在训练中更准确地估计动作的贡献。在Actor-Critic算法中，Actor的策略梯度可以通过以下方式计算：

$$ \nabla_{\theta} J(\theta) \approx \mathbb{E}_{\pi_{\theta}} \left[ \nabla_{\theta} \log \pi_{\theta}(a|s) \cdot A(s, a) \right] $$

其中，$\pi_{\theta}(a|s)$ 是在状态s下选择动作a的策略，$A(s, a)$ 是动作的优势。这样，优势函数可以帮助减小策略梯度的方差，提高训练的稳定性。
继而推导出策略梯度损失函数：
$$L^{PG}(\theta)=E_{t}[\operatorname{log}\pi_{\theta}(a_{t}|s_{t})*A_{t}]$$



优势学习的思想在训练过程中更加关注动作的相对贡献，而不仅仅是依赖于直接的即时奖励或价值估计，从而有助于更准确地引导Agent的学习。

但是这样的话就引进两个函数：Q函数和V函数，但是我们可以将Q函数替换掉：
![[Pasted image 20231204143813.png]]


