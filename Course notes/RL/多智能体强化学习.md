多智能体的情形相比于单智能体更加复杂，因为每个智能体在和环境交互的同时也在和其他智能体进行直接或者间接的交互。因此，多智能体强化学习要比单智能体更困难，其难点主要体现在以下几点：

- 由于多个智能体在环境中进行实时动态交互，并且每个智能体在不断学习并更新自身策略，因此在每个智能体的视角下，环境是**非稳态的**（non-stationary），即对于一个智能体而言，即使在相同的状态下采取相同的动作，得到的**状态转移和奖励信号的分布**可能在不断改变；（例如：在一个多智能体的游戏中，一个智能体的移动可能会改变其他智能体可以采取的行动或决策。或者他们需要相互协作等）
- 多个智能体的训练可能是多目标的，不同智能体需要最大化自己的利益；
- 训练评估的复杂度会增加，可能需要大规模分布式训练来提高效率。

# 基本求解范式
将多智能体强化学习（MARL）的基本求解范式划分为完全中心化和完全去中心化，可以提供一个清晰的视角来理解不同的方法如何处理智能体之间的交互和学习。下面是这两种范式的主要特点和区别：

### 完全中心化范式

1. **定义**：
   - 在完全中心化范式中，存在一个中心控制点，该点负责所有决策过程和信息处理。所有智能体的行为都由这个中心点直接控制。

2. **决策过程**：
   - 决策是基于全局信息的，即中心化控制器能够接收到所有智能体的状态和环境信息。

3. **学习和适应性**：
   - 学习过程也是中心化的，智能体作为一个整体进行学习，通常共享一个共同的策略或值函数。

4. **优点**：
   - 易于实现协调和一致性，因为所有决策都是集中做出的。
   - 学习过程中能够考虑所有智能体的信息，有助于寻找全局最优解。

5. **局限性**：
   - 可扩展性较差，难以应对大量智能体的情况。
   - 对于环境信息的处理和决策需要较大的计算资源。

### 完全去中心化范式

1. **定义**：
   - 在完全去中心化范式中，每个智能体独立地做出决策，没有中心化的控制点。

2. **决策过程**：
   - 每个智能体仅基于其局部信息或自己的观察来做决策。

3. **学习和适应性**：
   - 每个智能体拥有自己的学习过程，根据个体经验调整策略。

4. **优点**：
   - 高度可扩展，适用于大规模多智能体系统。
   - 智能体能够快速适应局部环境的变化。

5. **局限性**：
   - 缺乏全局视角可能导致次优决策。
   - 协调和合作难以实现，特别是在智能体之间缺乏有效沟通时。
   - 可能难以收敛。

在实际应用中，完全中心化和完全去中心化范式通常被看作是两个极端，实际系统往往采用这两种范式的某种组合，以取得平衡和适应不同的应用场景需求。例如，集中式训练与分散式执行（CTDE）就是一种结合了中心化训练和去中心化执行的流行方法。

# IPPO算法
IPPO（Independent Proximal Policy Optimization）算法是一种用于多智能体强化学习的**完全去中心化**算法，它基于单智能体强化学习中非常流行的Proximal Policy Optimization（PPO）算法。IPPO旨在处理多智能体环境中的协作和竞争问题，同时保持相对简单和高效的特点。

### 背景：PPO算法

首先，了解PPO是理解IPPO的关键。PPO是一种策略梯度方法，它优化一个“剪辑”目标函数，用于限制策略更新步骤中的变化幅度，以此来防止过大的策略更新导致性能急剧下降。PPO因其相对简单的实现和良好的性能而广受欢迎。

### IPPO的主要特点

1. **独立性**：
   - 在IPPO中，每个智能体都有自己**独立的策略网络**。这意味着每个智能体基于自己的观察来做决策，而不是依赖于其他智能体的状态或策略。

2. **策略优化**：
   - **类似于PPO，IPPO使用截断优化方法来更新策略**，这有助于避免由于过大的策略更新而导致的性能不稳定。

3. **适用性**：
   - IPPO适用于多种多智能体环境，无论是合作、竞争还是混合场景。

4. **简单性和效率**：
   - 相比于需要复杂协调和通信的多智能体算法，IPPO的独立性使得它在实现和训练上更为简单和高效。

5. **局限性**：
   - 由于每个智能体独立学习，它们可能无法有效地协调行动或学习到需要复杂合作的策略。
   - 在高度交互的多智能体环境中，IPPO可能不如那些专门设计用于协调和通信的算法性能好。

### 应用场景

IPPO由于其实现的简洁性和对PPO的直接扩展，适用于许多多智能体环境，尤其是那些环境动态性不太强，或者智能体间交互相对独立的场景。它在诸如多智能体协调、竞技游戏、资源分配等问题上都有应用潜力。

总的来说，IPPO是一个相对简单且有效的多智能体强化学习算法，它利用PPO的核心思想，在多智能体环境中实现了独立的策略学习。尽管它可能不适用于需要复杂协调和通信的场景，但在很多情况下，它仍然是一个有价值的选择。

## 流程图
![[IMG-20240110122829452.png]]


## 区分参数共享和非参数共享的IPPO
参数共享版的IPPO（Independent Proximal Policy Optimization）与非参数共享的IPPO在多智能体强化学习（MARL）中体现出不同的特点和应用方式。这两种方法的主要区别在于智能体如何使用和更新它们的策略网络。下面详细解释这两者之间的区别：

### 参数共享版的IPPO

在参数共享版的IPPO中，所有智能体使用相同的策略和/或价值网络参数。这意味着所有智能体在学习过程中共享相同的模型权重。

1. **优点**：
   - **计算效率**：由于所有智能体共享相同的网络，因此减少了所需的计算资源和存储空间。
   - **更快的学习**：共享参数可以使得网络从所有智能体的经验中学习，从而加快学习过程。
   - **统一的策略更新**：所有智能体的策略更新是一致的，保证了策略间的协调一致性。

2. **局限性**：
   - **适应性减弱**：在高度异质的环境中，共享参数可能限制了智能体根据自身特定情况进行个性化学习的能力。
   - **缺乏灵活性**：当环境或任务对智能体的要求差异较大时，共享参数可能导致性能下降。

### 非参数共享的IPPO

在非参数共享的IPPO中，每个智能体都有自己独立的策略和/或价值网络，每个网络的参数都是独立更新的。

1. **优点**：
   - **灵活性和适应性**：每个智能体可以根据自己的经验独立学习，更好地适应其特定的任务和环境。
   - **个性化策略**：允许智能体发展更加个性化的策略，特别是在智能体之间存在显著差异的情况下。

2. **局限性**：
   - **计算资源需求更高**：每个智能体有自己的网络，意味着需要更多的计算资源和存储空间。
   - **学习速度可能较慢**：由于不共享参数，智能体可能无法从其他智能体的经验中受益，导致学习速度较慢。

### 总结

选择参数共享还是非参数共享的IPPO取决于应用场景和特定需求。参数共享版的IPPO在处理相似或相同任务的智能体时更为高效，而非参数共享的IPPO在处理需要个性化策略或智能体间差异较大的任务时更为适合。在实际应用中，这个选择需要根据智能体的异质性、任务的复杂性以及计算资源的可用性来做出。

## 代码
```python
actor_lr = 3e-4
critic_lr = 1e-3
num_episodes = 100000
hidden_dim = 64
gamma = 0.99
lmbda = 0.97
eps = 0.2
device = torch.device("cuda") if torch.cuda.is_available() else torch.device(
    "cpu")

team_size = 2
grid_size = (15, 15)
#创建Combat环境，格子世界的大小为15x15，己方智能体和敌方智能体数量都为2
env = Combat(grid_shape=grid_size, n_agents=team_size, n_opponents=team_size)

state_dim = env.observation_space[0].shape[0]
action_dim = env.action_space[0].n
#两个智能体共享同一个策略
agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, eps,
            gamma, device)

win_list = []
for i in range(10):
    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:
        for i_episode in range(int(num_episodes / 10)):
            transition_dict_1 = {
                'states': [],
                'actions': [],
                'next_states': [],
                'rewards': [],
                'dones': []
            }
            transition_dict_2 = {
                'states': [],
                'actions': [],
                'next_states': [],
                'rewards': [],
                'dones': []
            }
            s = env.reset()
            terminal = False
            while not terminal:
                a_1 = agent.take_action(s[0])
                a_2 = agent.take_action(s[1])
                next_s, r, done, info = env.step([a_1, a_2])
                transition_dict_1['states'].append(s[0])
                transition_dict_1['actions'].append(a_1)
                transition_dict_1['next_states'].append(next_s[0])
                transition_dict_1['rewards'].append(
                    r[0] + 100 if info['win'] else r[0] - 0.1)
                transition_dict_1['dones'].append(False)
                transition_dict_2['states'].append(s[1])
                transition_dict_2['actions'].append(a_2)
                transition_dict_2['next_states'].append(next_s[1])
                transition_dict_2['rewards'].append(
                    r[1] + 100 if info['win'] else r[1] - 0.1)
                transition_dict_2['dones'].append(False)
                s = next_s
                terminal = all(done)
            win_list.append(1 if info["win"] else 0)
            agent.update(transition_dict_1)
            agent.update(transition_dict_2)
            if (i_episode + 1) % 100 == 0:
                pbar.set_postfix({
                    'episode':
                    '%d' % (num_episodes / 10 * i + i_episode + 1),
                    'return':
                    '%.3f' % np.mean(win_list[-100:])
                })
            pbar.update(1)

```

### 代码解释
这段代码是一个使用PPO（Proximal Policy Optimization）算法的强化学习实验示例，目标是训练智能体在一个名为“Combat”的多智能体环境中进行学习和决策。下面是代码的逐行解释：

1. **初始化超参数**：
   - `actor_lr` 和 `critic_lr`：分别是PPO算法中演员网络（actor network）和评论家网络（critic network）的学习率。
   - `num_episodes`：总的训练回合数。
   - `hidden_dim`：神经网络隐藏层的维度。
   - `gamma`：折扣因子，用于计算未来奖励的当前价值。
   - `lmbda`：用于GAE（Generalized Advantage Estimation）的参数。
   - `eps`：PPO算法中的剪辑参数，用于限制策略更新的幅度。
   - `device`：设定使用的计算设备，优先使用CUDA（GPU）。

2. **环境和智能体的设置**：
   - `team_size` 和 `grid_size`：设置Combat环境中的队伍大小和格子世界的尺寸。
   - `env`：创建Combat环境实例。
   - `state_dim` 和 `action_dim`：分别表示环境状态的维度和可用动作的数量。
   - `agent`：创建一个PPO智能体，用于学习和执行动作。

3. **训练循环**：
   - 循环进行`num_episodes`个回合的训练。
   - 对于每个回合，收集两个智能体的转换（状态、动作、下一个状态、奖励和完成标志）。
   - 智能体根据当前状态采取动作，环境返回下一个状态、奖励和是否完成回合的标志。
   - 奖励被修改，如果智能体赢得游戏（`info['win']`），奖励增加100，否则减少0.1。
   - 使用收集的转换信息更新智能体的策略。

4. **性能跟踪**：
   - `win_list`记录了每个回合的胜负结果，用于跟踪智能体的性能。
   - 每100个回合，打印出平均胜率。

这个脚本实现了一个基本的多智能体强化学习框架，其中两个智能体在Combat环境中独立学习和操作，但共享同一策略模型。这是一个参数共享的IPPO实现，因为尽管两个智能体各自经历不同的状态和动作序列，它们仍然使用相同的PPO策略进行学习和决策。

### 参数共享的体现

代码中体现参数共享的关键在于如何创建和使用`agent`对象，这是一个PPO智能体。在代码中，只创建了一个`agent`实例，并且这个实例被用于控制所有智能体的行为。具体来说，代码中的这些部分体现了参数共享：

1. **智能体的创建**：
   ```python
   agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, eps,
               gamma, device)
   ```
   这行代码创建了一个PPO智能体实例`agent`。此实例包含了演员（actor）和评论家（critic）网络的参数。

2. **智能体的决策过程**：
   ```python
   a_1 = agent.take_action(s[0])
   a_2 = agent.take_action(s[1])
   ```
   在这里，相同的`agent`实例被用来为两个不同的智能体（在这个环境中分别表示为`s[0]`和`s[1]`）决定行动。这意味着两个智能体在进行决策时使用的是相同的策略网络。

3. **智能体的学习过程**：
   ```python
   agent.update(transition_dict_1)
   agent.update(transition_dict_2)
   ```
   在训练过程中，相同的`agent`实例用于处理来自两个不同智能体的经验（通过`transition_dict_1`和`transition_dict_2`）。这意味着这些经验数据被用来更新同一个策略模型。

由于整个训练过程中所有智能体共享同一个PPO智能体实例，因此它们共享相同的策略和价值网络参数。这是参数共享的一个直接体现，意味着所有智能体在学习过程中共享信息和经验，而不是每个智能体拥有各自独立的策略模型。这种方法在多智能体环境中常用于提高学习效率和减少所需的计算资源。

# CTDE算法
在强化学习中，CTDE（Centralized Training with Decentralized Execution）是一种重要的算法框架，主要用于多智能体强化学习问题。在多智能体环境中，每个智能体需要学习如何在与其他智能体交互的同时优化自己的行为。这种情况下，如何有效地协调和学习是一个关键问题。CTDE提供了一种解决方案。

### CTDE的核心思想

CTDE的核心思想是在训练期间利用所有智能体的信息（集中式训练），但在执行或决策时，每个智能体只依赖于其自身的观察或信息（分散式执行）。这种方法的优点是在训练阶段可以充分利用环境中所有智能体的信息，从而更有效地学习协作策略，但在实际执行时不需要所有智能体的信息，增强了系统的可扩展性和实用性。

### CTDE的实现

1. **集中式训练**：在训练阶段，系统可以访问所有智能体的状态、行为和奖励信息。利用这些信息，系统可以训练一个或多个智能体，以学习如何在多智能体环境中有效协作。

2. **分散式执行**：在实际执行或测试阶段，每个智能体只能访问自己的局部观察信息。智能体需要根据自己的观察来做出决策，无法依赖其他智能体的信息。

### 应用场景

CTDE框架在多种多智能体环境中都有应用，例如自动驾驶汽车中的车辆协调、机器人足球比赛、多无人机协作控制等。

### 挑战

尽管CTDE在理论上很有吸引力，但在实践中也面临一些挑战，如维度的诅咒、策略表示的复杂性、通信和计算资源的限制等。

总之，CTDE是多智能体强化学习中一个非常重要的框架，它通过集中式训练和分散式执行的结合，有效地平衡了学习效率和执行的灵活性。


# MADDPG算法
MADDPG（Multi-Agent Deep Deterministic Policy Gradient）算法是一种在多智能体系统中广泛使用的强化学习方法。它是基于DDPG（Deep Deterministic Policy Gradient）算法的扩展，特别针对多智能体环境进行了设计和优化。

## DDPG的基础

要理解MADDPG，首先需要了解DDPG。DDPG是一种结合了深度学习和策略梯度的强化学习方法。它适用于连续动作空间，使用了一种称为演员-评论家（Actor-Critic）的框架。在这个框架中：
- **演员（Actor）** 负责学习选择动作。
- **评论家（Critic）** 负责评估演员选择的动作的价值。

DDPG使用深度神经网络来逼近演员和评论家的策略，利用**经验回放和目标网络**来稳定训练过程。

## MADDPG的特点

MADDPG在DDPG的基础上进行了扩展，以适应多智能体环境。其关键特点包括：

1. **多智能体学习**：MADDPG为每个智能体分别设计了演员和评论家网络。**每个智能体的Actor只能观察到自己的观察空间，并据此做出决策。而每个智能体的Critic可以访问所有智能体的信息，以更全面地评估动作的价值。**

2. **集中式训练，分散式执行**：这与CTDE（Centralized Training with Decentralized Execution）的思想一致。在训练期间，每个智能体的评论家可以访问整个环境的信息，但在实际执行时，每个智能体的演员只依赖于自己的观察。

3. **适用于连续动作空间**：MADDPG特别适用于需要连续动作决策的环境，这在多智能体场景中是常见的。

### 具体特征
在MADDPG（Multi-Agent Deep Deterministic Policy Gradient）算法中，智能体之间的观察和信息共享机制是其核心设计之一。具体来说：

1. **演员（Actor）网络**：在MADDPG框架下，每个智能体的演员网络仅能观察到自己的状态或者信息。这意味着在决策时，每个智能体的行为仅基于其自身的观察和知识。这种设计符合实际应用中的需求，例如在无人机编队飞行中，每个无人机只能根据自己的传感器数据来做出决策。

2. **评论家（Critic）网络**：与演员网络不同，MADDPG中的每个智能体的评论家网络可以访问所有智能体的状态信息，包括其他智能体的观察和动作。这种集中式的信息访问允许评论家更准确地评估和学习在多智能体环境中的动作价值。评论家网络通过这种方式有助于智能体学习在多智能体交互中的复杂策略。

因此，在MADDPG中，尽管智能体在执行阶段只能根据自己的观察来做出决策，它们在训练阶段可以利用其他智能体的状态和价值信息来更好地学习如何在多智能体环境中进行协作或竞争。这种设计有效地结合了分散式执行（Decentralized Execution）的灵活性和集中式训练（Centralized Training）的信息丰富性，是MADDPG处理多智能体问题的一个关键优势。


### 应用场景

MADDPG由于其高效的多智能体学习机制，在诸如机器人协调、自动驾驶汽车、智能游戏角色控制等领域都有广泛的应用。

### 挑战与局限

尽管MADDPG在多智能体强化学习领域非常有效，但它也面临一些挑战，如高维状态和动作空间的处理、多智能体协调的复杂性、以及算法的计算成本等。

总的来说，MADDPG是多智能体强化学习领域的一个重要算法，它通过在多智能体环境中扩展DDPG算法，提供了有效的学习机制来处理复杂的协作和竞争问题。


## 算法总览图
![[IMG-20240110180201351.png]]

## 代码
![[IMG-20240110180223090.png]]


```python
def onehot_from_logits(logits, eps=0.01):
    ''' 生成最优动作的独热（one-hot）形式 '''
    argmax_acs = (logits == logits.max(1, keepdim=True)[0]).float()
    # 生成随机动作,转换成独热形式
    rand_acs = torch.autograd.Variable(torch.eye(logits.shape[1])[[
        np.random.choice(range(logits.shape[1]), size=logits.shape[0])
    ]],
                                       requires_grad=False).to(logits.device)
    # 通过epsilon-贪婪算法来选择用哪个动作
    return torch.stack([
        argmax_acs[i] if r > eps else rand_acs[i]
        for i, r in enumerate(torch.rand(logits.shape[0]))
    ])


def sample_gumbel(shape, eps=1e-20, tens_type=torch.FloatTensor):
    """从Gumbel(0,1)分布中采样"""
    U = torch.autograd.Variable(tens_type(*shape).uniform_(),
                                requires_grad=False)
    return -torch.log(-torch.log(U + eps) + eps)


def gumbel_softmax_sample(logits, temperature):
    """ 从Gumbel-Softmax分布中采样"""
    y = logits + sample_gumbel(logits.shape, tens_type=type(logits.data)).to(
        logits.device)
    return F.softmax(y / temperature, dim=1)


def gumbel_softmax(logits, temperature=1.0):
    """从Gumbel-Softmax分布中采样,并进行离散化"""
    y = gumbel_softmax_sample(logits, temperature)
    y_hard = onehot_from_logits(y)
    y = (y_hard.to(logits.device) - y).detach() + y
    # 返回一个y_hard的独热量,但是它的梯度是y,我们既能够得到一个与环境交互的离散动作,又可以
    # 正确地反传梯度
    return y


class TwoLayerFC(torch.nn.Module):
    def __init__(self, num_in, num_out, hidden_dim):
        super().__init__()
        self.fc1 = torch.nn.Linear(num_in, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = torch.nn.Linear(hidden_dim, num_out)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


class DDPG:
    ''' DDPG算法 '''
    def __init__(self, state_dim, action_dim, critic_input_dim, hidden_dim,
                 actor_lr, critic_lr, device):
        self.actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)
        self.target_actor = TwoLayerFC(state_dim, action_dim,
                                       hidden_dim).to(device)
        self.critic = TwoLayerFC(critic_input_dim, 1, hidden_dim).to(device)
        self.target_critic = TwoLayerFC(critic_input_dim, 1,
                                        hidden_dim).to(device)
        self.target_critic.load_state_dict(self.critic.state_dict())
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),
                                                lr=actor_lr)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),
                                                 lr=critic_lr)

    def take_action(self, state, explore=False):
        action = self.actor(state)
        if explore:
            action = gumbel_softmax(action)
        else:
            action = onehot_from_logits(action)
        return action.detach().cpu().numpy()[0]

    def soft_update(self, net, target_net, tau):
        for param_target, param in zip(target_net.parameters(),
                                       net.parameters()):
            param_target.data.copy_(param_target.data * (1.0 - tau) +
                                    param.data * tau)

class MADDPG:
    def __init__(self, env, device, actor_lr, critic_lr, hidden_dim,
                 state_dims, action_dims, critic_input_dim, gamma, tau):
        self.agents = []
        for i in range(len(env.agents)):
            self.agents.append(
                DDPG(state_dims[i], action_dims[i], critic_input_dim,
                     hidden_dim, actor_lr, critic_lr, device))
        self.gamma = gamma
        self.tau = tau
        self.critic_criterion = torch.nn.MSELoss()
        self.device = device

    @property
    def policies(self):
        return [agt.actor for agt in self.agents]

    @property
    def target_policies(self):
        return [agt.target_actor for agt in self.agents]

    def take_action(self, states, explore):
        states = [
            torch.tensor([states[i]], dtype=torch.float, device=self.device)
            for i in range(len(env.agents))
        ]
        return [
            agent.take_action(state, explore)
            for agent, state in zip(self.agents, states)
        ]

    def update(self, sample, i_agent):
        obs, act, rew, next_obs, done = sample
        cur_agent = self.agents[i_agent]

        cur_agent.critic_optimizer.zero_grad()
        all_target_act = [
            onehot_from_logits(pi(_next_obs))
            for pi, _next_obs in zip(self.target_policies, next_obs)
        ]
        target_critic_input = torch.cat((*next_obs, *all_target_act), dim=1)
        target_critic_value = rew[i_agent].view(
            -1, 1) + self.gamma * cur_agent.target_critic(
                target_critic_input) * (1 - done[i_agent].view(-1, 1))
        critic_input = torch.cat((*obs, *act), dim=1)
        critic_value = cur_agent.critic(critic_input)
        critic_loss = self.critic_criterion(critic_value,
                                            target_critic_value.detach())
        critic_loss.backward()
        cur_agent.critic_optimizer.step()

        cur_agent.actor_optimizer.zero_grad()
        cur_actor_out = cur_agent.actor(obs[i_agent])
        cur_act_vf_in = gumbel_softmax(cur_actor_out)
        all_actor_acs = []
        for i, (pi, _obs) in enumerate(zip(self.policies, obs)):
            if i == i_agent:
                all_actor_acs.append(cur_act_vf_in)
            else:
                all_actor_acs.append(onehot_from_logits(pi(_obs)))
        vf_in = torch.cat((*obs, *all_actor_acs), dim=1)
        actor_loss = -cur_agent.critic(vf_in).mean()
        actor_loss += (cur_actor_out**2).mean() * 1e-3
        actor_loss.backward()
        cur_agent.actor_optimizer.step()

    def update_all_targets(self):
        for agt in self.agents:
            agt.soft_update(agt.actor, agt.target_actor, self.tau)
            agt.soft_update(agt.critic, agt.target_critic, self.tau)

num_episodes = 5000
episode_length = 25  # 每条序列的最大长度
buffer_size = 100000
hidden_dim = 64
actor_lr = 1e-2
critic_lr = 1e-2
gamma = 0.95
tau = 1e-2
batch_size = 1024
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
update_interval = 100
minimal_size = 4000

env_id = "simple_adversary"
env = make_env(env_id)
replay_buffer = rl_utils.ReplayBuffer(buffer_size)

state_dims = []
action_dims = []
for action_space in env.action_space:
    action_dims.append(action_space.n)
for state_space in env.observation_space:
    state_dims.append(state_space.shape[0])
critic_input_dim = sum(state_dims) + sum(action_dims)

maddpg = MADDPG(env, device, actor_lr, critic_lr, hidden_dim, state_dims,
                action_dims, critic_input_dim, gamma, tau)


def evaluate(env_id, maddpg, n_episode=10, episode_length=25):
    # 对学习的策略进行评估,此时不会进行探索
    env = make_env(env_id)
    returns = np.zeros(len(env.agents))
    for _ in range(n_episode):
        obs = env.reset()
        for t_i in range(episode_length):
            actions = maddpg.take_action(obs, explore=False)
            obs, rew, done, info = env.step(actions)
            rew = np.array(rew)
            returns += rew / n_episode
    return returns.tolist()


return_list = []  # 记录每一轮的回报（return）
total_step = 0
for i_episode in range(num_episodes):
    state = env.reset()
    # ep_returns = np.zeros(len(env.agents))
    for e_i in range(episode_length):
        actions = maddpg.take_action(state, explore=True)
        next_state, reward, done, _ = env.step(actions)
        replay_buffer.add(state, actions, reward, next_state, done)
        state = next_state

        total_step += 1
        if replay_buffer.size(
        ) >= minimal_size and total_step % update_interval == 0:
            sample = replay_buffer.sample(batch_size)

            def stack_array(x):
                rearranged = [[sub_x[i] for sub_x in x]
                              for i in range(len(x[0]))]
                return [
                    torch.FloatTensor(np.vstack(aa)).to(device)
                    for aa in rearranged
                ]

            sample = [stack_array(x) for x in sample]
            for a_i in range(len(env.agents)):
                maddpg.update(sample, a_i)
            maddpg.update_all_targets()
    if (i_episode + 1) % 100 == 0:
        ep_returns = evaluate(env_id, maddpg, n_episode=100)
        return_list.append(ep_returns)
        print(f"Episode: {i_episode+1}, {ep_returns}")


```



### 解析

好的，让我详细解释一下`DDPG`类和`MADDPG`类的代码。

#### DDPG类

`DDPG`（深度确定性策略梯度）是一种用于连续动作空间的强化学习算法。它结合了价值学习（如Q学习）和策略梯度的方法。在这个类中：

- **初始化方法** (`__init__`):
  - 初始化了两个主要的网络：策略网络（`self.actor`）和价值网络（`self.critic`）。这两个网络都使用了前面定义的`TwoLayerFC`类来创建两层全连接网络。
  - 还初始化了两个目标网络（`self.target_actor` 和 `self.target_critic`），它们的结构与策略网络和价值网络相同。目标网络用于稳定学习过程。
  - 设置了优化器（`self.actor_optimizer` 和 `self.critic_optimizer`），用于优化策略和价值网络的参数。

- **take_action 方法**:
  - 接受当前状态作为输入，通过策略网络生成一个动作。
  - 如果设置了`explore`为True，它将使用`gumbel_softmax`来添加探索噪声，否则使用`onehot_from_logits`来生成确定的动作。

- **soft_update 方法**:
  - 用于更新目标网络的参数。它实现了一种称为软更新的技术，即目标网络参数是主网络参数和原目标网络参数的加权平均。这有助于使学习过程更加稳定。

#### MADDPG类

`MADDPG`（多智能体深度确定性策略梯度）是DDPG的一个扩展，专门用于多智能体环境。它处理多个智能体同时学习和互动的情况。在这个类中：

- **初始化方法** (`__init__`):
  - 初始化多个DDPG智能体（`self.agents`），每个智能体都有自己的策略和价值网络。
  - 设置了折扣因子`gamma`和软更新参数`tau`。
  - 初始化了用于训练的损失函数（`self.critic_criterion`）。

- **policies 和 target_policies 属性**:
  - 这两个属性提供了一个方便的方式来获取所有智能体的策略网络和目标策略网络的列表。

- **take_action 方法**:
  - 对于给定的状态，此方法生成所有智能体的动作。它为每个智能体调用`take_action`方法。

- **update 方法**:
  - 用于更新一个特定智能体的网络参数。它接受一个经验样本（包括观察、动作、奖励等），并进行一系列的计算，以优化策略和价值网络。
  - 首先，它计算目标价值网络的输出，然后计算策略网络的损失，并使用这些信息来更新智能体的网络。

- **update_all_targets 方法**:
  - 调用`soft_update`方法更新所有智能体的目标网络。

总的来说，`DDPG`类实现了单智能体在连续动作空间中的深度强化学习算法，而`MADDPG`类将这个算法扩展到多智能体环境中，使得多个智能体可以在同一个环境中相互作用并学习。

### 集中式训练，分布式执行
好的，让我详细对应到代码段来解释“集中式训练，分散式执行”是如何实现的。

#### 集中式训练（Centralized Training）

在`MADDPG`类中，集中式训练主要体现在以下几个方面：

1. **共享信息的训练过程**（在`update`方法中体现）：
   - 当更新一个智能体的网络时，会考虑其他所有智能体的策略。例如，在计算目标价值时，会使用所有智能体的目标策略网络（`target_policies`）和所有智能体的下一个观察状态（`next_obs`）。
   ```python
   all_target_act = [
       onehot_from_logits(pi(_next_obs))
       for pi, _next_obs in zip(self.target_policies, next_obs)
   ]
   ```
   - 在计算当前智能体的策略损失时，也会考虑其他智能体的当前策略（`self.policies`）。
   ```python
   all_actor_acs = []
   for i, (pi, _obs) in enumerate(zip(self.policies, obs)):
       if i == i_agent:
           all_actor_acs.append(cur_act_vf_in)
       else:
           all_actor_acs.append(onehot_from_logits(pi(_obs)))
   ```

2. **经验回放缓冲区的使用**（在主训练循环中体现）：
   - 所有智能体共享同一个经验回放缓冲区（`replay_buffer`），这意味着在学习过程中，每个智能体都可以访问由所有智能体共同产生的经验数据。
   ```python
   replay_buffer.add(state, actions, reward, next_state, done)
   ```

#### 分散式执行（Decentralized Execution）

在`MADDPG`类中，分散式执行体现在`take_action`方法中：

1. **独立的动作决策**：
   - 每个智能体根据自己的策略网络和当前观察来独立地决定其动作。在实际执行动作时，不会使用其他智能体的信息。
   ```python
   actions = [
       agent.take_action(state, explore)
       for agent, state in zip(self.agents, states)
   ]
   ```
   - 在`take_action`方法中，每个智能体仅使用自己的策略网络（`self.actor`）来生成动作。
   ```python
   action = self.actor(state)
   ```

通过这种方式，`MADDPG`算法在训练阶段利用所有智能体的信息来进行集中式学习，而在实际执行阶段，每个智能体则仅依赖于自己的观察和策略进行独立的决策。这样的设计允许智能体在训练时充分考虑其他智能体的可能行为，而在执行时能够灵活适应各自的观察情况，从而在多智能体环境中实现有效的协作和竞争。